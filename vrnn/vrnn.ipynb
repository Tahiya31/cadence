{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains Variational Recurrent Neural Network model for learning feature representation that helps to find changes points on time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from model import VRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25 ## length of the sliding window to create samples/sequences\n",
    "trn_ratio = 0.7\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 608 Number of variables: 3 Train-Test splitting ratio: 426\n",
      "number_of_training_samples: 401 number_of_testing_samples: 182\n"
     ]
    }
   ],
   "source": [
    "dataset = sio.loadmat('./beedance/beedance-6.mat')\n",
    "        \n",
    "Y = dataset['Y']                # Y: time series data, time length (T) x number of variables (D) => T x D\n",
    "L = dataset['L']                # L: label of change-point, time length (T) x 1 (labeled by 0 or 1)\n",
    "T, D = Y.shape                  # T: time length; D: number of variables\n",
    "    \n",
    "n_trn = int(np.ceil(T * trn_ratio))  # splitting point\n",
    "print('Length of dataset:', T, 'Number of variables:', D, 'Train-Test splitting ratio:', n_trn)\n",
    "    \n",
    "train_set_idx = range(window_size, n_trn)\n",
    "test_set_idx = range(n_trn, T)\n",
    "\n",
    "print('number_of_training_samples:', len(train_set_idx), 'number_of_testing_samples:', len(test_set_idx)) ## number of train samples, number of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(idx, window_size, Y, L): \n",
    "    n = len(idx)\n",
    "    A = torch.zeros((n, D))       \n",
    "    B = torch.zeros((n, 1))\n",
    "    X_p = torch.zeros((n, window_size, D)) ## past samples sequence set created by sliding window\n",
    "    X_f = torch.zeros((n, window_size, D)) ## future samples sequence set created by sliding window\n",
    "\n",
    "    for i in range(n):\n",
    "        l = idx[i] - window_size\n",
    "        m = idx[i]\n",
    "        n = idx[i] - window_size + 1\n",
    "\n",
    "        X_p[i, :, :] = torch.from_numpy(Y[l:m, :])\n",
    "        X_f[i, :, :] = torch.from_numpy(Y[n:m+1, :])\n",
    "            \n",
    "        A[i, :] = torch.from_numpy(Y[m, :])\n",
    "        \n",
    "        B[i, :] = torch.from_numpy(L[m])\n",
    "            \n",
    "\n",
    "    data, future_data, true_data, labels = Variable(X_p), Variable(X_f), Variable(A), Variable(B)\n",
    "            \n",
    "    return data, future_data, true_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sequenced Samples for Training: torch.Size([401, 25, 3])\n",
      "Shape of Sequenced Samples for Testing: torch.Size([182, 25, 3])\n",
      "Shape of Original Train Data: torch.Size([401, 3])\n",
      "Shape of Original Test Data: torch.Size([182, 3])\n",
      "Shape of Training Labels: torch.Size([401, 1])\n",
      "Shape of Testing Labels: torch.Size([182, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, future_train, true_train, labels_train = train_test(train_set_idx, window_size, Y, L)\n",
    "test_dataset, future_test, true_test, labels_test = train_test(test_set_idx, window_size, Y, L)\n",
    "\n",
    "print('Shape of Sequenced Samples for Training:', train_dataset.size())\n",
    "print('Shape of Sequenced Samples for Testing:', test_dataset.size())\n",
    "print('Shape of Original Train Data:', true_train.size())\n",
    "print('Shape of Original Test Data:', true_test.size())\n",
    "print('Shape of Training Labels:', labels_train.size())\n",
    "print('Shape of Testing Labels:', labels_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401, 75])\n",
      "torch.Size([182, 75])\n"
     ]
    }
   ],
   "source": [
    "## For flattening observations in a single window\n",
    "\n",
    "def flatten(data, idx, window_size, dim):\n",
    "    return data.reshape((len(idx), window_size*D))\n",
    "\n",
    "train_flat = flatten(train_dataset, train_set_idx, window_size, D)\n",
    "test_flat = flatten(test_dataset, test_set_idx, window_size, D)\n",
    "print(train_flat.size())\n",
    "print(test_flat.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "x_dim = 3\n",
    "h_dim = 100\n",
    "z_dim = 1\n",
    "n_layers =  1\n",
    "n_epochs = 100\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "seed = 128\n",
    "print_every = 100\n",
    "save_every = 10\n",
    "\n",
    "#manual seed\n",
    "torch.manual_seed(seed)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network\n",
    "class phi_x(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(phi_x, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(x_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class phi_z(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim):\n",
    "        super(phi_z, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, h_dim, z_dim):\n",
    "        super(encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(h_dim + h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, h_dim, x_dim):\n",
    "        super(decoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(h_dim + h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, x_dim),\n",
    "            nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, bias = False):\n",
    "        super(Model, self).__init__()\n",
    "        self.Phi_x = phi_x(x_dim, h_dim)\n",
    "        self.Phi_z = phi_z(z_dim, h_dim)\n",
    "        \n",
    "        self.Encoder = encoder(h_dim, z_dim)\n",
    "        self.Enc_mean = nn.Linear(z_dim, z_dim)\n",
    "        self.Enc_std = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Softplus())\n",
    "        \n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.ReLU())\n",
    "        self.prior_mean = nn.Linear(z_dim, z_dim)\n",
    "        self.prior_std = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Softplus())\n",
    "\n",
    "        self.Decoder = decoder(h_dim, x_dim)\n",
    "        self.dec_std = nn.Sequential(\n",
    "            nn.Linear(x_dim, x_dim),\n",
    "            nn.Softplus())\n",
    "        #self.dec_mean = nn.Linear(h_dim, x_dim)\n",
    "        self.dec_mean = nn.Sequential(\n",
    "            nn.Linear(x_dim, x_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.rnn = nn.GRU(h_dim + h_dim, h_dim, n_layers, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        all_enc_mean, all_enc_std, all_enc_t = [], [], []\n",
    "        all_dec_mean, all_dec_std, all_dec_t = [], [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "\n",
    "        h = Variable(torch.zeros(n_layers, x.size(1), h_dim))\n",
    "\n",
    "        for t in range(x.size(0)):\n",
    "\n",
    "            phi_x_t = self.Phi_x(x[t])\n",
    "\n",
    "            #encoder\n",
    "            enc_t = self.Encoder(torch.cat([phi_x_t, h[-1]], 1))\n",
    "            enc_mean_t = self.Enc_mean(enc_t)\n",
    "            enc_std_t = self.Enc_std(enc_t)\n",
    "\n",
    "            #prior\n",
    "            prior_t = self.prior(h[-1])\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "\n",
    "            #sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "            phi_z_t = self.Phi_z(z_t)\n",
    "\n",
    "            #decoder\n",
    "            dec_t = self.Decoder(torch.cat([phi_z_t, h[-1]], 1))\n",
    "            dec_mean_t = self.dec_mean(dec_t)\n",
    "            dec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "            #recurrence\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
    "\n",
    "            #computing losses\n",
    "            kld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
    "            nll_loss += self._L2_norm(dec_mean_t, x[t])\n",
    "            \n",
    "            \n",
    "            all_enc_t.append(enc_t)\n",
    "            all_enc_std.append(enc_std_t)\n",
    "            all_enc_mean.append(enc_mean_t)\n",
    "            all_dec_t.append(dec_t)\n",
    "            all_dec_mean.append(dec_mean_t)\n",
    "            all_dec_std.append(dec_std_t)\n",
    "        \n",
    "        #print(all_enc_t)\n",
    "        encoded = self._helper_dim(all_enc_t)\n",
    "        #print(\"Encoded:\", encoded)\n",
    "        mmd_loss = self._mmd_loss(encoded)\n",
    "        #print(\"MMD:\", mmd_loss)\n",
    "        #print(\"Enc:\", encoded.size())\n",
    "        \n",
    "        return kld_loss, nll_loss, mmd_loss, \\\n",
    "            (all_enc_t, all_enc_mean, all_enc_std), \\\n",
    "            (all_dec_t, all_dec_mean, all_dec_std)\n",
    "    \n",
    "    ## To generate encoded or reconstructed sequence within training\n",
    "    def sample(self, window_size):\n",
    "        \n",
    "        sample = torch.zeros(x.size(0), self.x_dim)\n",
    "        \n",
    "        h = Variable(torch.zeros(self.n_layers, x.size(1), self.h_dim))\n",
    "        \n",
    "        for t in range(x.size(0)):\n",
    "            \n",
    "            phi_x_t = self.Phi_x(x[t]) \n",
    "            \n",
    "            #encoder\n",
    "            enc_t = self.Encoder(torch.cat([phi_x_t, h[-1]], 1))\n",
    "            enc_mean_t = self.Enc_mean(enc_t)\n",
    "            enc_std_t = self.enc_std(enc_t)\n",
    "\n",
    "            phi_z_t = self.Phi_z(enc_mean_t)\n",
    "            \n",
    "            #recurrence\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
    "            \n",
    "            sample[t] = enc_mean_t.data\n",
    "            \n",
    "        return sample\n",
    "            \n",
    "    def _reparameterized_sample(self, mean, std):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "        \"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "        kld_element =  (2 * torch.log(std_2) - 2 * torch.log(std_1) + \n",
    "            (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "            std_2.pow(2) - 1)\n",
    "        return 0.5 * torch.sum(kld_element)\n",
    "    \n",
    "    def _L2_norm(self, mean, x):    ##using L2 norm distance between decoder mean and x\n",
    "        return torch.sum(torch.dist(x, mean, 2))\n",
    "    \n",
    "    ## Create past and future samples encoding \n",
    "    def _past_future(self, x):\n",
    "        \n",
    "        x_size = x.size(0)\n",
    "        #print(x_size)\n",
    "        dim = x.size(1)\n",
    "        #print(dim)\n",
    "        \n",
    "        X_p = torch.zeros(x.size(0), x.size(1), x.size(2)) ## past samples sequence set created by sliding window\n",
    "        X_f = torch.zeros(x.size(0), x.size(1), x.size(2)) ## future samples sequence set created by sliding window\n",
    "        \n",
    "        for i in range((x_size-1)):\n",
    "            X_p[i, :, :] = x[i, :, :]\n",
    "        \n",
    "        for i in range(1, x_size):\n",
    "            X_f[i, :, :] = x[i, :, :]\n",
    "        \n",
    "        #print(\"Past:\", X_p)\n",
    "        #print(\"Future:\", X_f)\n",
    "        \n",
    "        return X_p, X_f\n",
    "    \n",
    "    ##helper to compute kernel for mmd\n",
    "    def compute_kernel(self, x, y):\n",
    "        dim = x.size(1)\n",
    "                                          \n",
    "        kernel_input = (x - y).pow(2).mean(2)/float(dim)\n",
    "        return torch.exp(-kernel_input)\n",
    "    \n",
    "    ## computes mmd loss between current and future sample\n",
    "    def _mmd_loss(self, x):\n",
    "        \n",
    "        X_p, X_f = self._past_future(x)\n",
    "        p_kernel = self.compute_kernel(X_p,X_p)\n",
    "        f_kernel = self.compute_kernel(X_f,X_f)\n",
    "        pf_kernel = self.compute_kernel(X_p,X_f)\n",
    "        mmd = p_kernel.mean() + f_kernel.mean() - 2*pf_kernel.mean()\n",
    "        \n",
    "        return torch.sum(mmd)\n",
    "    \n",
    "     ## computes kmeans loss between encoded samples and cluster centroids\n",
    "    def _kmeans_loss(self, x):\n",
    "        kmeans = KMeans(n_cluster = 8, random_state = 0).fit(x)\n",
    "        loss = kmeans.inertia_\n",
    "        return torch.sum(loss) \n",
    "    \n",
    "    \n",
    "    def _helper_dim(self, x):\n",
    "        x = [t.detach().numpy() for t in x]\n",
    "        x = torch.FloatTensor(x)\n",
    "        x = Variable(x.transpose(1, 0))\n",
    "        #print(\"Enc:\", x.size())\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss, total_kld_loss, total_nll_loss, total_mmd_loss = 0, 0, 0, 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        #print(data.size())\n",
    "        data = Variable(data.squeeze().transpose(0, 1))\n",
    "        #print(data.size())\n",
    "        #data = (data - data.min().data) / (data.max().data - data.min().data)\n",
    "\n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        kld_loss, nll_loss, mmd_loss, _, _ = model(data)\n",
    "        loss = kld_loss + nll_loss - mmd_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #grad norm clipping, only in pytorch version >= 1.10\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        train_loss += loss.data\n",
    "        total_kld_loss += kld_loss.data\n",
    "        total_nll_loss += nll_loss.data\n",
    "        total_mmd_loss +=mmd_loss.data\n",
    "    \n",
    "        #printing\n",
    "        \n",
    "        print('Train Epoch: {} Batch: {} KLD Loss: {:.6f} \\t NLL Loss: {:.6f} \\t MMD Loss: {:.6f}'.format(\n",
    "        epoch, batch_idx, kld_loss.data / batch_size, nll_loss.data / batch_size, mmd_loss.data / batch_size))\n",
    "\n",
    "        #sample = model.sample(28)\n",
    "        #plt.imshow(sample.numpy())\n",
    "        #plt.pause(1e-6)\n",
    "    \n",
    "    print('====> Epoch: {} Train set loss: KLD Loss: {:.6f} \\t NLL Loss: {:.6f} \\t MMD Loss: {:.6f} \\t Average loss: {:.4f}'.format(\n",
    "        epoch, total_kld_loss/len(train_loader.dataset), total_nll_loss/len(train_loader.dataset), total_mmd_loss/len(train_loader.dataset), train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "\n",
    "    mean_kld_loss, mean_nll_loss, mean_mmd_loss = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(test_loader):                                            \n",
    "\n",
    "        #data = Variable(data)\n",
    "        data = Variable(data.squeeze().transpose(0, 1))\n",
    "        #data = (data - data.min().data) / (data.max().data - data.min().data)\n",
    "\n",
    "        kld_loss, nll_loss, mmd_loss, _, _ = model(data)\n",
    "        \n",
    "        mean_kld_loss += kld_loss.data\n",
    "        mean_nll_loss += nll_loss.data\n",
    "        mean_mmd_loss += mmd_loss.data\n",
    "\n",
    "    mean_kld_loss /= len(test_loader.dataset)\n",
    "    mean_nll_loss /= len(test_loader.dataset)\n",
    "    mean_mmd_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: KLD Loss = {:.4f} \\t NLL Loss = {:.4f} \\t MMD Loss = {:.4f}'.format(\n",
    "    mean_kld_loss, mean_nll_loss, mean_mmd_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x_dim, h_dim, z_dim, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 0 KLD Loss: 56.953384 \t NLL Loss: 1.530206 \t MMD Loss: 0.000000\n",
      "Train Epoch: 1 Batch: 1 KLD Loss: 41.092838 \t NLL Loss: 1.397996 \t MMD Loss: 0.000000\n",
      "Train Epoch: 1 Batch: 2 KLD Loss: 40.800335 \t NLL Loss: 1.284521 \t MMD Loss: 0.000001\n",
      "Train Epoch: 1 Batch: 3 KLD Loss: 40.542126 \t NLL Loss: 1.192774 \t MMD Loss: 0.000001\n",
      "Train Epoch: 1 Batch: 4 KLD Loss: 40.230148 \t NLL Loss: 1.090812 \t MMD Loss: 0.000001\n",
      "Train Epoch: 1 Batch: 5 KLD Loss: 39.937630 \t NLL Loss: 1.102498 \t MMD Loss: 0.000000\n",
      "====> Epoch: 1 Train set loss: KLD Loss: 41.425468 \t NLL Loss: 1.212777 \t MMD Loss: 0.000000 \t Average loss: 42.6382\n",
      "====> Test set loss: KLD Loss = 27.9124 \t NLL Loss = 0.8199 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_1.pth\n",
      "Train Epoch: 2 Batch: 0 KLD Loss: 39.664265 \t NLL Loss: 1.443227 \t MMD Loss: 0.000000\n",
      "Train Epoch: 2 Batch: 1 KLD Loss: 39.448792 \t NLL Loss: 1.301998 \t MMD Loss: 0.000000\n",
      "Train Epoch: 2 Batch: 2 KLD Loss: 39.158455 \t NLL Loss: 1.221601 \t MMD Loss: 0.000000\n",
      "Train Epoch: 2 Batch: 3 KLD Loss: 38.862484 \t NLL Loss: 1.116367 \t MMD Loss: 0.000001\n",
      "Train Epoch: 2 Batch: 4 KLD Loss: 38.611809 \t NLL Loss: 1.055445 \t MMD Loss: 0.000001\n",
      "Train Epoch: 2 Batch: 5 KLD Loss: 38.349892 \t NLL Loss: 1.083281 \t MMD Loss: 0.000001\n",
      "====> Epoch: 2 Train set loss: KLD Loss: 37.361904 \t NLL Loss: 1.152625 \t MMD Loss: 0.000001 \t Average loss: 38.5145\n",
      "====> Test set loss: KLD Loss = 26.7892 \t NLL Loss = 0.8194 \t MMD Loss = 0.0000\n",
      "Train Epoch: 3 Batch: 0 KLD Loss: 38.157215 \t NLL Loss: 1.437878 \t MMD Loss: 0.000002\n",
      "Train Epoch: 3 Batch: 1 KLD Loss: 37.800941 \t NLL Loss: 1.299509 \t MMD Loss: 0.000001\n",
      "Train Epoch: 3 Batch: 2 KLD Loss: 37.540710 \t NLL Loss: 1.217066 \t MMD Loss: 0.000001\n",
      "Train Epoch: 3 Batch: 3 KLD Loss: 37.316433 \t NLL Loss: 1.103139 \t MMD Loss: 0.000001\n",
      "Train Epoch: 3 Batch: 4 KLD Loss: 37.037670 \t NLL Loss: 1.034336 \t MMD Loss: 0.000001\n",
      "Train Epoch: 3 Batch: 5 KLD Loss: 36.780689 \t NLL Loss: 1.065113 \t MMD Loss: 0.000001\n",
      "====> Epoch: 3 Train set loss: KLD Loss: 35.851757 \t NLL Loss: 1.142271 \t MMD Loss: 0.000001 \t Average loss: 36.9940\n",
      "====> Test set loss: KLD Loss = 25.6596 \t NLL Loss = 0.8059 \t MMD Loss = 0.0000\n",
      "Train Epoch: 4 Batch: 0 KLD Loss: 36.479362 \t NLL Loss: 1.448711 \t MMD Loss: 0.000001\n",
      "Train Epoch: 4 Batch: 1 KLD Loss: 36.219971 \t NLL Loss: 1.310358 \t MMD Loss: 0.000001\n",
      "Train Epoch: 4 Batch: 2 KLD Loss: 35.966465 \t NLL Loss: 1.219264 \t MMD Loss: 0.000001\n",
      "Train Epoch: 4 Batch: 3 KLD Loss: 35.707108 \t NLL Loss: 1.088967 \t MMD Loss: 0.000001\n",
      "Train Epoch: 4 Batch: 4 KLD Loss: 35.454292 \t NLL Loss: 1.016780 \t MMD Loss: 0.000001\n",
      "Train Epoch: 4 Batch: 5 KLD Loss: 35.191628 \t NLL Loss: 1.050682 \t MMD Loss: 0.000001\n",
      "====> Epoch: 4 Train set loss: KLD Loss: 34.317219 \t NLL Loss: 1.138715 \t MMD Loss: 0.000001 \t Average loss: 35.4559\n",
      "====> Test set loss: KLD Loss = 24.5686 \t NLL Loss = 0.7940 \t MMD Loss = 0.0000\n",
      "Train Epoch: 5 Batch: 0 KLD Loss: 34.942570 \t NLL Loss: 1.455907 \t MMD Loss: 0.000002\n",
      "Train Epoch: 5 Batch: 1 KLD Loss: 34.679737 \t NLL Loss: 1.319273 \t MMD Loss: 0.000001\n",
      "Train Epoch: 5 Batch: 2 KLD Loss: 34.438530 \t NLL Loss: 1.221217 \t MMD Loss: 0.000001\n",
      "Train Epoch: 5 Batch: 3 KLD Loss: 34.190742 \t NLL Loss: 1.079237 \t MMD Loss: 0.000001\n",
      "Train Epoch: 5 Batch: 4 KLD Loss: 33.922016 \t NLL Loss: 1.002421 \t MMD Loss: 0.000001\n",
      "Train Epoch: 5 Batch: 5 KLD Loss: 33.668907 \t NLL Loss: 1.038988 \t MMD Loss: 0.000002\n",
      "====> Epoch: 5 Train set loss: KLD Loss: 32.852669 \t NLL Loss: 1.135887 \t MMD Loss: 0.000001 \t Average loss: 33.9886\n",
      "====> Test set loss: KLD Loss = 23.5024 \t NLL Loss = 0.7861 \t MMD Loss = 0.0000\n",
      "Train Epoch: 6 Batch: 0 KLD Loss: 33.436340 \t NLL Loss: 1.460871 \t MMD Loss: 0.000002\n",
      "Train Epoch: 6 Batch: 1 KLD Loss: 33.168915 \t NLL Loss: 1.323313 \t MMD Loss: 0.000002\n",
      "Train Epoch: 6 Batch: 2 KLD Loss: 32.915653 \t NLL Loss: 1.220575 \t MMD Loss: 0.000002\n",
      "Train Epoch: 6 Batch: 3 KLD Loss: 32.666431 \t NLL Loss: 1.071640 \t MMD Loss: 0.000002\n",
      "Train Epoch: 6 Batch: 4 KLD Loss: 32.417664 \t NLL Loss: 0.996253 \t MMD Loss: 0.000002\n",
      "Train Epoch: 6 Batch: 5 KLD Loss: 32.177132 \t NLL Loss: 1.035050 \t MMD Loss: 0.000002\n",
      "====> Epoch: 6 Train set loss: KLD Loss: 31.406626 \t NLL Loss: 1.134396 \t MMD Loss: 0.000002 \t Average loss: 32.5410\n",
      "====> Test set loss: KLD Loss = 22.4538 \t NLL Loss = 0.7831 \t MMD Loss = 0.0000\n",
      "Train Epoch: 7 Batch: 0 KLD Loss: 31.922119 \t NLL Loss: 1.461424 \t MMD Loss: 0.000002\n",
      "Train Epoch: 7 Batch: 1 KLD Loss: 31.679737 \t NLL Loss: 1.321179 \t MMD Loss: 0.000002\n",
      "Train Epoch: 7 Batch: 2 KLD Loss: 31.434860 \t NLL Loss: 1.217836 \t MMD Loss: 0.000002\n",
      "Train Epoch: 7 Batch: 3 KLD Loss: 31.191576 \t NLL Loss: 1.068486 \t MMD Loss: 0.000002\n",
      "Train Epoch: 7 Batch: 4 KLD Loss: 30.948435 \t NLL Loss: 0.992153 \t MMD Loss: 0.000003\n",
      "Train Epoch: 7 Batch: 5 KLD Loss: 30.706575 \t NLL Loss: 1.031529 \t MMD Loss: 0.000003\n",
      "====> Epoch: 7 Train set loss: KLD Loss: 29.986362 \t NLL Loss: 1.131987 \t MMD Loss: 0.000002 \t Average loss: 31.1183\n",
      "====> Test set loss: KLD Loss = 21.4263 \t NLL Loss = 0.7827 \t MMD Loss = 0.0000\n",
      "Train Epoch: 8 Batch: 0 KLD Loss: 30.478300 \t NLL Loss: 1.456973 \t MMD Loss: 0.000003\n",
      "Train Epoch: 8 Batch: 1 KLD Loss: 30.223156 \t NLL Loss: 1.317150 \t MMD Loss: 0.000003\n",
      "Train Epoch: 8 Batch: 2 KLD Loss: 29.987612 \t NLL Loss: 1.213193 \t MMD Loss: 0.000003\n",
      "Train Epoch: 8 Batch: 3 KLD Loss: 29.753576 \t NLL Loss: 1.065552 \t MMD Loss: 0.000003\n",
      "Train Epoch: 8 Batch: 4 KLD Loss: 29.509008 \t NLL Loss: 0.994258 \t MMD Loss: 0.000003\n",
      "Train Epoch: 8 Batch: 5 KLD Loss: 29.272678 \t NLL Loss: 1.032470 \t MMD Loss: 0.000003\n",
      "====> Epoch: 8 Train set loss: KLD Loss: 28.604380 \t NLL Loss: 1.129911 \t MMD Loss: 0.000003 \t Average loss: 29.7343\n",
      "====> Test set loss: KLD Loss = 20.4204 \t NLL Loss = 0.7843 \t MMD Loss = 0.0000\n",
      "Train Epoch: 9 Batch: 0 KLD Loss: 29.038464 \t NLL Loss: 1.451721 \t MMD Loss: 0.000004\n",
      "Train Epoch: 9 Batch: 1 KLD Loss: 28.799112 \t NLL Loss: 1.309839 \t MMD Loss: 0.000004\n",
      "Train Epoch: 9 Batch: 2 KLD Loss: 28.565214 \t NLL Loss: 1.207542 \t MMD Loss: 0.000004\n",
      "Train Epoch: 9 Batch: 3 KLD Loss: 28.332424 \t NLL Loss: 1.063754 \t MMD Loss: 0.000004\n",
      "Train Epoch: 9 Batch: 4 KLD Loss: 28.097958 \t NLL Loss: 0.995436 \t MMD Loss: 0.000004\n",
      "Train Epoch: 9 Batch: 5 KLD Loss: 27.868086 \t NLL Loss: 1.034620 \t MMD Loss: 0.000004\n",
      "====> Epoch: 9 Train set loss: KLD Loss: 27.244091 \t NLL Loss: 1.127248 \t MMD Loss: 0.000003 \t Average loss: 28.3713\n",
      "====> Test set loss: KLD Loss = 19.4360 \t NLL Loss = 0.7869 \t MMD Loss = 0.0000\n",
      "Train Epoch: 10 Batch: 0 KLD Loss: 27.634720 \t NLL Loss: 1.444993 \t MMD Loss: 0.000004\n",
      "Train Epoch: 10 Batch: 1 KLD Loss: 27.407358 \t NLL Loss: 1.300942 \t MMD Loss: 0.000004\n",
      "Train Epoch: 10 Batch: 2 KLD Loss: 27.175957 \t NLL Loss: 1.202108 \t MMD Loss: 0.000004\n",
      "Train Epoch: 10 Batch: 3 KLD Loss: 26.948454 \t NLL Loss: 1.062700 \t MMD Loss: 0.000004\n",
      "Train Epoch: 10 Batch: 4 KLD Loss: 26.723431 \t NLL Loss: 0.997187 \t MMD Loss: 0.000005\n",
      "Train Epoch: 10 Batch: 5 KLD Loss: 26.493370 \t NLL Loss: 1.036585 \t MMD Loss: 0.000005\n",
      "====> Epoch: 10 Train set loss: KLD Loss: 25.916536 \t NLL Loss: 1.124312 \t MMD Loss: 0.000004 \t Average loss: 27.0408\n",
      "====> Test set loss: KLD Loss = 18.4735 \t NLL Loss = 0.7894 \t MMD Loss = 0.0000\n",
      "Train Epoch: 11 Batch: 0 KLD Loss: 26.267653 \t NLL Loss: 1.439630 \t MMD Loss: 0.000005\n",
      "Train Epoch: 11 Batch: 1 KLD Loss: 26.047155 \t NLL Loss: 1.292818 \t MMD Loss: 0.000004\n",
      "Train Epoch: 11 Batch: 2 KLD Loss: 25.821419 \t NLL Loss: 1.196388 \t MMD Loss: 0.000005\n",
      "Train Epoch: 11 Batch: 3 KLD Loss: 25.595896 \t NLL Loss: 1.060793 \t MMD Loss: 0.000005\n",
      "Train Epoch: 11 Batch: 4 KLD Loss: 25.373405 \t NLL Loss: 0.997836 \t MMD Loss: 0.000005\n",
      "Train Epoch: 11 Batch: 5 KLD Loss: 25.153788 \t NLL Loss: 1.036852 \t MMD Loss: 0.000005\n",
      "====> Epoch: 11 Train set loss: KLD Loss: 24.619942 \t NLL Loss: 1.121088 \t MMD Loss: 0.000005 \t Average loss: 25.7410\n",
      "====> Test set loss: KLD Loss = 17.5348 \t NLL Loss = 0.7922 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_11.pth\n",
      "Train Epoch: 12 Batch: 0 KLD Loss: 24.938253 \t NLL Loss: 1.432547 \t MMD Loss: 0.000006\n",
      "Train Epoch: 12 Batch: 1 KLD Loss: 24.711353 \t NLL Loss: 1.283526 \t MMD Loss: 0.000006\n",
      "Train Epoch: 12 Batch: 2 KLD Loss: 24.495817 \t NLL Loss: 1.191023 \t MMD Loss: 0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 Batch: 3 KLD Loss: 24.281490 \t NLL Loss: 1.061072 \t MMD Loss: 0.000005\n",
      "Train Epoch: 12 Batch: 4 KLD Loss: 24.060392 \t NLL Loss: 1.000438 \t MMD Loss: 0.000005\n",
      "Train Epoch: 12 Batch: 5 KLD Loss: 23.843250 \t NLL Loss: 1.038216 \t MMD Loss: 0.000006\n",
      "====> Epoch: 12 Train set loss: KLD Loss: 23.354502 \t NLL Loss: 1.118296 \t MMD Loss: 0.000005 \t Average loss: 24.4728\n",
      "====> Test set loss: KLD Loss = 16.6185 \t NLL Loss = 0.7949 \t MMD Loss = 0.0000\n",
      "Train Epoch: 13 Batch: 0 KLD Loss: 23.636600 \t NLL Loss: 1.426310 \t MMD Loss: 0.000007\n",
      "Train Epoch: 13 Batch: 1 KLD Loss: 23.415993 \t NLL Loss: 1.275309 \t MMD Loss: 0.000007\n",
      "Train Epoch: 13 Batch: 2 KLD Loss: 23.200533 \t NLL Loss: 1.185202 \t MMD Loss: 0.000006\n",
      "Train Epoch: 13 Batch: 3 KLD Loss: 22.991215 \t NLL Loss: 1.059527 \t MMD Loss: 0.000006\n",
      "Train Epoch: 13 Batch: 4 KLD Loss: 22.780973 \t NLL Loss: 1.003762 \t MMD Loss: 0.000006\n",
      "Train Epoch: 13 Batch: 5 KLD Loss: 22.568144 \t NLL Loss: 1.040017 \t MMD Loss: 0.000007\n",
      "====> Epoch: 13 Train set loss: KLD Loss: 22.119654 \t NLL Loss: 1.115631 \t MMD Loss: 0.000006 \t Average loss: 23.2353\n",
      "====> Test set loss: KLD Loss = 15.7233 \t NLL Loss = 0.7972 \t MMD Loss = 0.0000\n",
      "Train Epoch: 14 Batch: 0 KLD Loss: 22.356792 \t NLL Loss: 1.418998 \t MMD Loss: 0.000007\n",
      "Train Epoch: 14 Batch: 1 KLD Loss: 22.149017 \t NLL Loss: 1.266207 \t MMD Loss: 0.000007\n",
      "Train Epoch: 14 Batch: 2 KLD Loss: 21.943272 \t NLL Loss: 1.179741 \t MMD Loss: 0.000008\n",
      "Train Epoch: 14 Batch: 3 KLD Loss: 21.735258 \t NLL Loss: 1.058332 \t MMD Loss: 0.000008\n",
      "Train Epoch: 14 Batch: 4 KLD Loss: 21.527971 \t NLL Loss: 1.004321 \t MMD Loss: 0.000008\n",
      "Train Epoch: 14 Batch: 5 KLD Loss: 21.325291 \t NLL Loss: 1.041153 \t MMD Loss: 0.000007\n",
      "====> Epoch: 14 Train set loss: KLD Loss: 20.913731 \t NLL Loss: 1.112220 \t MMD Loss: 0.000007 \t Average loss: 22.0259\n",
      "====> Test set loss: KLD Loss = 14.8541 \t NLL Loss = 0.7981 \t MMD Loss = 0.0000\n",
      "Train Epoch: 15 Batch: 0 KLD Loss: 21.124218 \t NLL Loss: 1.413374 \t MMD Loss: 0.000007\n",
      "Train Epoch: 15 Batch: 1 KLD Loss: 20.918478 \t NLL Loss: 1.258069 \t MMD Loss: 0.000008\n",
      "Train Epoch: 15 Batch: 2 KLD Loss: 20.717657 \t NLL Loss: 1.174343 \t MMD Loss: 0.000008\n",
      "Train Epoch: 15 Batch: 3 KLD Loss: 20.521889 \t NLL Loss: 1.056350 \t MMD Loss: 0.000009\n",
      "Train Epoch: 15 Batch: 4 KLD Loss: 20.315527 \t NLL Loss: 1.003765 \t MMD Loss: 0.000009\n",
      "Train Epoch: 15 Batch: 5 KLD Loss: 20.116524 \t NLL Loss: 1.041604 \t MMD Loss: 0.000008\n",
      "====> Epoch: 15 Train set loss: KLD Loss: 19.744925 \t NLL Loss: 1.108829 \t MMD Loss: 0.000008 \t Average loss: 20.8537\n",
      "====> Test set loss: KLD Loss = 14.0110 \t NLL Loss = 0.7992 \t MMD Loss = 0.0000\n",
      "Train Epoch: 16 Batch: 0 KLD Loss: 19.922453 \t NLL Loss: 1.407119 \t MMD Loss: 0.000008\n",
      "Train Epoch: 16 Batch: 1 KLD Loss: 19.726976 \t NLL Loss: 1.250463 \t MMD Loss: 0.000008\n",
      "Train Epoch: 16 Batch: 2 KLD Loss: 19.523653 \t NLL Loss: 1.168410 \t MMD Loss: 0.000009\n",
      "Train Epoch: 16 Batch: 3 KLD Loss: 19.331539 \t NLL Loss: 1.052751 \t MMD Loss: 0.000010\n",
      "Train Epoch: 16 Batch: 4 KLD Loss: 19.142851 \t NLL Loss: 1.004157 \t MMD Loss: 0.000010\n",
      "Train Epoch: 16 Batch: 5 KLD Loss: 18.943069 \t NLL Loss: 1.041046 \t MMD Loss: 0.000010\n",
      "====> Epoch: 16 Train set loss: KLD Loss: 18.607967 \t NLL Loss: 1.105069 \t MMD Loss: 0.000009 \t Average loss: 19.7130\n",
      "====> Test set loss: KLD Loss = 13.1867 \t NLL Loss = 0.7993 \t MMD Loss = 0.0000\n",
      "Train Epoch: 17 Batch: 0 KLD Loss: 18.751783 \t NLL Loss: 1.402159 \t MMD Loss: 0.000010\n",
      "Train Epoch: 17 Batch: 1 KLD Loss: 18.564081 \t NLL Loss: 1.242436 \t MMD Loss: 0.000010\n",
      "Train Epoch: 17 Batch: 2 KLD Loss: 18.380573 \t NLL Loss: 1.163319 \t MMD Loss: 0.000009\n",
      "Train Epoch: 17 Batch: 3 KLD Loss: 18.188805 \t NLL Loss: 1.051135 \t MMD Loss: 0.000010\n",
      "Train Epoch: 17 Batch: 4 KLD Loss: 17.991137 \t NLL Loss: 1.004453 \t MMD Loss: 0.000010\n",
      "Train Epoch: 17 Batch: 5 KLD Loss: 17.809904 \t NLL Loss: 1.039029 \t MMD Loss: 0.000011\n",
      "====> Epoch: 17 Train set loss: KLD Loss: 17.506039 \t NLL Loss: 1.101651 \t MMD Loss: 0.000010 \t Average loss: 18.6077\n",
      "====> Test set loss: KLD Loss = 12.3978 \t NLL Loss = 0.8001 \t MMD Loss = 0.0000\n",
      "Train Epoch: 18 Batch: 0 KLD Loss: 17.667360 \t NLL Loss: 1.394549 \t MMD Loss: 0.000013\n",
      "Train Epoch: 18 Batch: 1 KLD Loss: 17.435799 \t NLL Loss: 1.235867 \t MMD Loss: 0.000012\n",
      "Train Epoch: 18 Batch: 2 KLD Loss: 17.260117 \t NLL Loss: 1.157459 \t MMD Loss: 0.000010\n",
      "Train Epoch: 18 Batch: 3 KLD Loss: 17.104214 \t NLL Loss: 1.049230 \t MMD Loss: 0.000010\n",
      "Train Epoch: 18 Batch: 4 KLD Loss: 16.909292 \t NLL Loss: 1.004347 \t MMD Loss: 0.000010\n",
      "Train Epoch: 18 Batch: 5 KLD Loss: 16.706480 \t NLL Loss: 1.038231 \t MMD Loss: 0.000011\n",
      "====> Epoch: 18 Train set loss: KLD Loss: 16.452190 \t NLL Loss: 1.098004 \t MMD Loss: 0.000011 \t Average loss: 17.5502\n",
      "====> Test set loss: KLD Loss = 11.6275 \t NLL Loss = 0.7994 \t MMD Loss = 0.0000\n",
      "Train Epoch: 19 Batch: 0 KLD Loss: 16.548428 \t NLL Loss: 1.388075 \t MMD Loss: 0.000013\n",
      "Train Epoch: 19 Batch: 1 KLD Loss: 16.370407 \t NLL Loss: 1.225187 \t MMD Loss: 0.000014\n",
      "Train Epoch: 19 Batch: 2 KLD Loss: 16.174313 \t NLL Loss: 1.151257 \t MMD Loss: 0.000013\n",
      "Train Epoch: 19 Batch: 3 KLD Loss: 15.991386 \t NLL Loss: 1.045412 \t MMD Loss: 0.000012\n",
      "Train Epoch: 19 Batch: 4 KLD Loss: 15.826812 \t NLL Loss: 1.005108 \t MMD Loss: 0.000012\n",
      "Train Epoch: 19 Batch: 5 KLD Loss: 15.650647 \t NLL Loss: 1.038291 \t MMD Loss: 0.000012\n",
      "====> Epoch: 19 Train set loss: KLD Loss: 15.411391 \t NLL Loss: 1.093798 \t MMD Loss: 0.000012 \t Average loss: 16.5052\n",
      "====> Test set loss: KLD Loss = 10.8778 \t NLL Loss = 0.7984 \t MMD Loss = 0.0000\n",
      "Train Epoch: 20 Batch: 0 KLD Loss: 15.471724 \t NLL Loss: 1.379830 \t MMD Loss: 0.000012\n",
      "Train Epoch: 20 Batch: 1 KLD Loss: 15.292919 \t NLL Loss: 1.215655 \t MMD Loss: 0.000014\n",
      "Train Epoch: 20 Batch: 2 KLD Loss: 15.145747 \t NLL Loss: 1.144256 \t MMD Loss: 0.000015\n",
      "Train Epoch: 20 Batch: 3 KLD Loss: 14.966865 \t NLL Loss: 1.040926 \t MMD Loss: 0.000015\n",
      "Train Epoch: 20 Batch: 4 KLD Loss: 14.781257 \t NLL Loss: 1.001757 \t MMD Loss: 0.000014\n",
      "Train Epoch: 20 Batch: 5 KLD Loss: 14.624157 \t NLL Loss: 1.035522 \t MMD Loss: 0.000013\n",
      "====> Epoch: 20 Train set loss: KLD Loss: 14.409204 \t NLL Loss: 1.088151 \t MMD Loss: 0.000013 \t Average loss: 15.4973\n",
      "====> Test set loss: KLD Loss = 10.1687 \t NLL Loss = 0.7950 \t MMD Loss = 0.0000\n",
      "Train Epoch: 21 Batch: 0 KLD Loss: 14.487935 \t NLL Loss: 1.370495 \t MMD Loss: 0.000012\n",
      "Train Epoch: 21 Batch: 1 KLD Loss: 14.297817 \t NLL Loss: 1.205463 \t MMD Loss: 0.000013\n",
      "Train Epoch: 21 Batch: 2 KLD Loss: 14.119474 \t NLL Loss: 1.137959 \t MMD Loss: 0.000015\n",
      "Train Epoch: 21 Batch: 3 KLD Loss: 13.991400 \t NLL Loss: 1.034296 \t MMD Loss: 0.000017\n",
      "Train Epoch: 21 Batch: 4 KLD Loss: 13.822117 \t NLL Loss: 0.995699 \t MMD Loss: 0.000017\n",
      "Train Epoch: 21 Batch: 5 KLD Loss: 13.628687 \t NLL Loss: 1.029310 \t MMD Loss: 0.000016\n",
      "====> Epoch: 21 Train set loss: KLD Loss: 13.461935 \t NLL Loss: 1.081013 \t MMD Loss: 0.000015 \t Average loss: 14.5429\n",
      "====> Test set loss: KLD Loss = 9.4729 \t NLL Loss = 0.7896 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_21.pth\n",
      "Train Epoch: 22 Batch: 0 KLD Loss: 13.469403 \t NLL Loss: 1.361222 \t MMD Loss: 0.000015\n",
      "Train Epoch: 22 Batch: 1 KLD Loss: 13.338270 \t NLL Loss: 1.194417 \t MMD Loss: 0.000014\n",
      "Train Epoch: 22 Batch: 2 KLD Loss: 13.160245 \t NLL Loss: 1.128564 \t MMD Loss: 0.000015\n",
      "Train Epoch: 22 Batch: 3 KLD Loss: 12.989176 \t NLL Loss: 1.028648 \t MMD Loss: 0.000016\n",
      "Train Epoch: 22 Batch: 4 KLD Loss: 12.840597 \t NLL Loss: 0.992459 \t MMD Loss: 0.000017\n",
      "Train Epoch: 22 Batch: 5 KLD Loss: 12.690842 \t NLL Loss: 1.023015 \t MMD Loss: 0.000018\n",
      "====> Epoch: 22 Train set loss: KLD Loss: 12.526848 \t NLL Loss: 1.073847 \t MMD Loss: 0.000015 \t Average loss: 13.6007\n",
      "====> Test set loss: KLD Loss = 8.8101 \t NLL Loss = 0.7818 \t MMD Loss = 0.0000\n",
      "Train Epoch: 23 Batch: 0 KLD Loss: 12.586481 \t NLL Loss: 1.349283 \t MMD Loss: 0.000019\n",
      "Train Epoch: 23 Batch: 1 KLD Loss: 12.367830 \t NLL Loss: 1.177431 \t MMD Loss: 0.000018\n",
      "Train Epoch: 23 Batch: 2 KLD Loss: 12.249731 \t NLL Loss: 1.116520 \t MMD Loss: 0.000015\n",
      "Train Epoch: 23 Batch: 3 KLD Loss: 12.153127 \t NLL Loss: 1.018924 \t MMD Loss: 0.000014\n",
      "Train Epoch: 23 Batch: 4 KLD Loss: 11.958867 \t NLL Loss: 0.988765 \t MMD Loss: 0.000015\n",
      "Train Epoch: 23 Batch: 5 KLD Loss: 11.767861 \t NLL Loss: 1.017088 \t MMD Loss: 0.000018\n",
      "====> Epoch: 23 Train set loss: KLD Loss: 11.664262 \t NLL Loss: 1.064221 \t MMD Loss: 0.000016 \t Average loss: 12.7285\n",
      "====> Test set loss: KLD Loss = 8.1866 \t NLL Loss = 0.7749 \t MMD Loss = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 Batch: 0 KLD Loss: 11.719301 \t NLL Loss: 1.325171 \t MMD Loss: 0.000021\n",
      "Train Epoch: 24 Batch: 1 KLD Loss: 11.534922 \t NLL Loss: 1.149005 \t MMD Loss: 0.000022\n",
      "Train Epoch: 24 Batch: 2 KLD Loss: 11.332535 \t NLL Loss: 1.098651 \t MMD Loss: 0.000020\n",
      "Train Epoch: 24 Batch: 3 KLD Loss: 11.191261 \t NLL Loss: 1.011583 \t MMD Loss: 0.000018\n",
      "Train Epoch: 24 Batch: 4 KLD Loss: 11.074192 \t NLL Loss: 0.985517 \t MMD Loss: 0.000017\n",
      "Train Epoch: 24 Batch: 5 KLD Loss: 10.925132 \t NLL Loss: 1.007060 \t MMD Loss: 0.000017\n",
      "====> Epoch: 24 Train set loss: KLD Loss: 10.817331 \t NLL Loss: 1.049694 \t MMD Loss: 0.000018 \t Average loss: 11.8670\n",
      "====> Test set loss: KLD Loss = 7.5661 \t NLL Loss = 0.7583 \t MMD Loss = 0.0000\n",
      "Train Epoch: 25 Batch: 0 KLD Loss: 10.769022 \t NLL Loss: 1.297104 \t MMD Loss: 0.000018\n",
      "Train Epoch: 25 Batch: 1 KLD Loss: 10.614747 \t NLL Loss: 1.120022 \t MMD Loss: 0.000019\n",
      "Train Epoch: 25 Batch: 2 KLD Loss: 10.496508 \t NLL Loss: 1.080453 \t MMD Loss: 0.000022\n",
      "Train Epoch: 25 Batch: 3 KLD Loss: 10.402411 \t NLL Loss: 0.988513 \t MMD Loss: 0.000024\n",
      "Train Epoch: 25 Batch: 4 KLD Loss: 10.233559 \t NLL Loss: 0.959337 \t MMD Loss: 0.000024\n",
      "Train Epoch: 25 Batch: 5 KLD Loss: 10.068029 \t NLL Loss: 0.984668 \t MMD Loss: 0.000021\n",
      "====> Epoch: 25 Train set loss: KLD Loss: 9.988513 \t NLL Loss: 1.026250 \t MMD Loss: 0.000020 \t Average loss: 11.0147\n",
      "====> Test set loss: KLD Loss = 6.9890 \t NLL Loss = 0.7366 \t MMD Loss = 0.0000\n",
      "Train Epoch: 26 Batch: 0 KLD Loss: 9.985491 \t NLL Loss: 1.255477 \t MMD Loss: 0.000018\n",
      "Train Epoch: 26 Batch: 1 KLD Loss: 9.874499 \t NLL Loss: 1.068270 \t MMD Loss: 0.000018\n",
      "Train Epoch: 26 Batch: 2 KLD Loss: 9.682385 \t NLL Loss: 1.050238 \t MMD Loss: 0.000020\n",
      "Train Epoch: 26 Batch: 3 KLD Loss: 9.533726 \t NLL Loss: 0.964119 \t MMD Loss: 0.000022\n",
      "Train Epoch: 26 Batch: 4 KLD Loss: 9.422935 \t NLL Loss: 0.934851 \t MMD Loss: 0.000024\n",
      "Train Epoch: 26 Batch: 5 KLD Loss: 9.298632 \t NLL Loss: 0.948322 \t MMD Loss: 0.000024\n",
      "====> Epoch: 26 Train set loss: KLD Loss: 9.224566 \t NLL Loss: 0.992922 \t MMD Loss: 0.000020 \t Average loss: 10.2175\n",
      "====> Test set loss: KLD Loss = 6.4384 \t NLL Loss = 0.6959 \t MMD Loss = 0.0000\n",
      "Train Epoch: 27 Batch: 0 KLD Loss: 9.211560 \t NLL Loss: 1.212967 \t MMD Loss: 0.000025\n",
      "Train Epoch: 27 Batch: 1 KLD Loss: 9.021670 \t NLL Loss: 1.025035 \t MMD Loss: 0.000024\n",
      "Train Epoch: 27 Batch: 2 KLD Loss: 8.911350 \t NLL Loss: 1.020054 \t MMD Loss: 0.000021\n",
      "Train Epoch: 27 Batch: 3 KLD Loss: 8.843158 \t NLL Loss: 0.928292 \t MMD Loss: 0.000020\n",
      "Train Epoch: 27 Batch: 4 KLD Loss: 8.719378 \t NLL Loss: 0.892913 \t MMD Loss: 0.000020\n",
      "Train Epoch: 27 Batch: 5 KLD Loss: 8.545989 \t NLL Loss: 0.925605 \t MMD Loss: 0.000021\n",
      "====> Epoch: 27 Train set loss: KLD Loss: 8.499249 \t NLL Loss: 0.958383 \t MMD Loss: 0.000021 \t Average loss: 9.4576\n",
      "====> Test set loss: KLD Loss = 5.9094 \t NLL Loss = 0.6820 \t MMD Loss = 0.0000\n",
      "Train Epoch: 28 Batch: 0 KLD Loss: 8.423821 \t NLL Loss: 1.136673 \t MMD Loss: 0.000025\n",
      "Train Epoch: 28 Batch: 1 KLD Loss: 8.320960 \t NLL Loss: 0.922287 \t MMD Loss: 0.000027\n",
      "Train Epoch: 28 Batch: 2 KLD Loss: 8.191161 \t NLL Loss: 0.969279 \t MMD Loss: 0.000027\n",
      "Train Epoch: 28 Batch: 3 KLD Loss: 8.055523 \t NLL Loss: 0.921350 \t MMD Loss: 0.000026\n",
      "Train Epoch: 28 Batch: 4 KLD Loss: 7.927094 \t NLL Loss: 0.890376 \t MMD Loss: 0.000025\n",
      "Train Epoch: 28 Batch: 5 KLD Loss: 7.817548 \t NLL Loss: 0.925298 \t MMD Loss: 0.000024\n",
      "====> Epoch: 28 Train set loss: KLD Loss: 7.778331 \t NLL Loss: 0.920142 \t MMD Loss: 0.000025 \t Average loss: 8.6984\n",
      "====> Test set loss: KLD Loss = 5.4176 \t NLL Loss = 0.6784 \t MMD Loss = 0.0000\n",
      "Train Epoch: 29 Batch: 0 KLD Loss: 7.787363 \t NLL Loss: 1.059381 \t MMD Loss: 0.000022\n",
      "Train Epoch: 29 Batch: 1 KLD Loss: 7.648112 \t NLL Loss: 0.838115 \t MMD Loss: 0.000022\n",
      "Train Epoch: 29 Batch: 2 KLD Loss: 7.474744 \t NLL Loss: 0.937553 \t MMD Loss: 0.000025\n",
      "Train Epoch: 29 Batch: 3 KLD Loss: 7.398510 \t NLL Loss: 0.906577 \t MMD Loss: 0.000030\n",
      "Train Epoch: 29 Batch: 4 KLD Loss: 7.306666 \t NLL Loss: 0.859119 \t MMD Loss: 0.000030\n",
      "Train Epoch: 29 Batch: 5 KLD Loss: 7.160115 \t NLL Loss: 0.908790 \t MMD Loss: 0.000029\n",
      "====> Epoch: 29 Train set loss: KLD Loss: 7.146216 \t NLL Loss: 0.879327 \t MMD Loss: 0.000025 \t Average loss: 8.0255\n",
      "====> Test set loss: KLD Loss = 4.9440 \t NLL Loss = 0.6730 \t MMD Loss = 0.0000\n",
      "Train Epoch: 30 Batch: 0 KLD Loss: 7.041943 \t NLL Loss: 1.024817 \t MMD Loss: 0.000028\n",
      "Train Epoch: 30 Batch: 1 KLD Loss: 6.922291 \t NLL Loss: 0.802091 \t MMD Loss: 0.000027\n",
      "Train Epoch: 30 Batch: 2 KLD Loss: 6.847621 \t NLL Loss: 0.921507 \t MMD Loss: 0.000025\n",
      "Train Epoch: 30 Batch: 3 KLD Loss: 6.801782 \t NLL Loss: 0.902845 \t MMD Loss: 0.000023\n",
      "Train Epoch: 30 Batch: 4 KLD Loss: 6.688520 \t NLL Loss: 0.856444 \t MMD Loss: 0.000023\n",
      "Train Epoch: 30 Batch: 5 KLD Loss: 6.526357 \t NLL Loss: 0.908752 \t MMD Loss: 0.000025\n",
      "====> Epoch: 30 Train set loss: KLD Loss: 6.516272 \t NLL Loss: 0.864472 \t MMD Loss: 0.000024 \t Average loss: 7.3807\n",
      "====> Test set loss: KLD Loss = 4.5047 \t NLL Loss = 0.6726 \t MMD Loss = 0.0000\n",
      "Train Epoch: 31 Batch: 0 KLD Loss: 6.496171 \t NLL Loss: 0.992402 \t MMD Loss: 0.000032\n",
      "Train Epoch: 31 Batch: 1 KLD Loss: 6.389716 \t NLL Loss: 0.782286 \t MMD Loss: 0.000034\n",
      "Train Epoch: 31 Batch: 2 KLD Loss: 6.226283 \t NLL Loss: 0.919472 \t MMD Loss: 0.000032\n",
      "Train Epoch: 31 Batch: 3 KLD Loss: 6.105648 \t NLL Loss: 0.899585 \t MMD Loss: 0.000030\n",
      "Train Epoch: 31 Batch: 4 KLD Loss: 6.008022 \t NLL Loss: 0.852324 \t MMD Loss: 0.000029\n",
      "Train Epoch: 31 Batch: 5 KLD Loss: 5.919351 \t NLL Loss: 0.907085 \t MMD Loss: 0.000028\n",
      "====> Epoch: 31 Train set loss: KLD Loss: 5.928410 \t NLL Loss: 0.854369 \t MMD Loss: 0.000029 \t Average loss: 6.7827\n",
      "====> Test set loss: KLD Loss = 4.0932 \t NLL Loss = 0.6704 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_31.pth\n",
      "Train Epoch: 32 Batch: 0 KLD Loss: 5.933263 \t NLL Loss: 0.991182 \t MMD Loss: 0.000025\n",
      "Train Epoch: 32 Batch: 1 KLD Loss: 5.820968 \t NLL Loss: 0.773109 \t MMD Loss: 0.000024\n",
      "Train Epoch: 32 Batch: 2 KLD Loss: 5.644555 \t NLL Loss: 0.910255 \t MMD Loss: 0.000029\n",
      "Train Epoch: 32 Batch: 3 KLD Loss: 5.602018 \t NLL Loss: 0.898619 \t MMD Loss: 0.000035\n",
      "Train Epoch: 32 Batch: 4 KLD Loss: 5.548964 \t NLL Loss: 0.849584 \t MMD Loss: 0.000037\n",
      "Train Epoch: 32 Batch: 5 KLD Loss: 5.398871 \t NLL Loss: 0.908899 \t MMD Loss: 0.000035\n",
      "====> Epoch: 32 Train set loss: KLD Loss: 5.418237 \t NLL Loss: 0.850936 \t MMD Loss: 0.000030 \t Average loss: 6.2691\n",
      "====> Test set loss: KLD Loss = 3.7048 \t NLL Loss = 0.6701 \t MMD Loss = 0.0000\n",
      "Train Epoch: 33 Batch: 0 KLD Loss: 5.263761 \t NLL Loss: 0.985489 \t MMD Loss: 0.000031\n",
      "Train Epoch: 33 Batch: 1 KLD Loss: 5.184006 \t NLL Loss: 0.763875 \t MMD Loss: 0.000030\n",
      "Train Epoch: 33 Batch: 2 KLD Loss: 5.115271 \t NLL Loss: 0.905466 \t MMD Loss: 0.000029\n",
      "Train Epoch: 33 Batch: 3 KLD Loss: 5.059854 \t NLL Loss: 0.893549 \t MMD Loss: 0.000028\n",
      "Train Epoch: 33 Batch: 4 KLD Loss: 4.970407 \t NLL Loss: 0.850566 \t MMD Loss: 0.000027\n",
      "Train Epoch: 33 Batch: 5 KLD Loss: 4.850148 \t NLL Loss: 0.903903 \t MMD Loss: 0.000030\n",
      "====> Epoch: 33 Train set loss: KLD Loss: 4.858805 \t NLL Loss: 0.846340 \t MMD Loss: 0.000028 \t Average loss: 5.7051\n",
      "====> Test set loss: KLD Loss = 3.3422 \t NLL Loss = 0.6700 \t MMD Loss = 0.0000\n",
      "Train Epoch: 34 Batch: 0 KLD Loss: 4.835625 \t NLL Loss: 0.969689 \t MMD Loss: 0.000036\n",
      "Train Epoch: 34 Batch: 1 KLD Loss: 4.758310 \t NLL Loss: 0.766972 \t MMD Loss: 0.000039\n",
      "Train Epoch: 34 Batch: 2 KLD Loss: 4.613252 \t NLL Loss: 0.905690 \t MMD Loss: 0.000036\n",
      "Train Epoch: 34 Batch: 3 KLD Loss: 4.508526 \t NLL Loss: 0.890941 \t MMD Loss: 0.000033\n",
      "Train Epoch: 34 Batch: 4 KLD Loss: 4.439737 \t NLL Loss: 0.847820 \t MMD Loss: 0.000031\n",
      "Train Epoch: 34 Batch: 5 KLD Loss: 4.365247 \t NLL Loss: 0.903926 \t MMD Loss: 0.000031\n",
      "====> Epoch: 34 Train set loss: KLD Loss: 4.392331 \t NLL Loss: 0.843497 \t MMD Loss: 0.000033 \t Average loss: 5.2358\n",
      "====> Test set loss: KLD Loss = 3.0096 \t NLL Loss = 0.6680 \t MMD Loss = 0.0000\n",
      "Train Epoch: 35 Batch: 0 KLD Loss: 4.284050 \t NLL Loss: 0.979501 \t MMD Loss: 0.000032\n",
      "Train Epoch: 35 Batch: 1 KLD Loss: 4.208265 \t NLL Loss: 0.759728 \t MMD Loss: 0.000032\n",
      "Train Epoch: 35 Batch: 2 KLD Loss: 4.121174 \t NLL Loss: 0.895969 \t MMD Loss: 0.000034\n",
      "Train Epoch: 35 Batch: 3 KLD Loss: 4.059216 \t NLL Loss: 0.887182 \t MMD Loss: 0.000038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 Batch: 4 KLD Loss: 4.000736 \t NLL Loss: 0.846259 \t MMD Loss: 0.000039\n",
      "Train Epoch: 35 Batch: 5 KLD Loss: 3.915587 \t NLL Loss: 0.900674 \t MMD Loss: 0.000037\n",
      "====> Epoch: 35 Train set loss: KLD Loss: 3.924433 \t NLL Loss: 0.840987 \t MMD Loss: 0.000034 \t Average loss: 4.7654\n",
      "====> Test set loss: KLD Loss = 2.6959 \t NLL Loss = 0.6676 \t MMD Loss = 0.0000\n",
      "Train Epoch: 36 Batch: 0 KLD Loss: 3.833172 \t NLL Loss: 0.963902 \t MMD Loss: 0.000035\n",
      "Train Epoch: 36 Batch: 1 KLD Loss: 3.772415 \t NLL Loss: 0.753271 \t MMD Loss: 0.000033\n",
      "Train Epoch: 36 Batch: 2 KLD Loss: 3.694412 \t NLL Loss: 0.897574 \t MMD Loss: 0.000034\n",
      "Train Epoch: 36 Batch: 3 KLD Loss: 3.619857 \t NLL Loss: 0.888873 \t MMD Loss: 0.000036\n",
      "Train Epoch: 36 Batch: 4 KLD Loss: 3.551162 \t NLL Loss: 0.845275 \t MMD Loss: 0.000036\n",
      "Train Epoch: 36 Batch: 5 KLD Loss: 3.484432 \t NLL Loss: 0.899034 \t MMD Loss: 0.000037\n",
      "====> Epoch: 36 Train set loss: KLD Loss: 3.504112 \t NLL Loss: 0.837575 \t MMD Loss: 0.000034 \t Average loss: 4.3417\n",
      "====> Test set loss: KLD Loss = 2.4049 \t NLL Loss = 0.6665 \t MMD Loss = 0.0000\n",
      "Train Epoch: 37 Batch: 0 KLD Loss: 3.421651 \t NLL Loss: 0.962824 \t MMD Loss: 0.000038\n",
      "Train Epoch: 37 Batch: 1 KLD Loss: 3.354793 \t NLL Loss: 0.746938 \t MMD Loss: 0.000038\n",
      "Train Epoch: 37 Batch: 2 KLD Loss: 3.289791 \t NLL Loss: 0.888635 \t MMD Loss: 0.000038\n",
      "Train Epoch: 37 Batch: 3 KLD Loss: 3.226934 \t NLL Loss: 0.881967 \t MMD Loss: 0.000037\n",
      "Train Epoch: 37 Batch: 4 KLD Loss: 3.165694 \t NLL Loss: 0.843100 \t MMD Loss: 0.000037\n",
      "Train Epoch: 37 Batch: 5 KLD Loss: 3.104613 \t NLL Loss: 0.897514 \t MMD Loss: 0.000037\n",
      "====> Epoch: 37 Train set loss: KLD Loss: 3.122350 \t NLL Loss: 0.833273 \t MMD Loss: 0.000036 \t Average loss: 3.9556\n",
      "====> Test set loss: KLD Loss = 2.1400 \t NLL Loss = 0.6653 \t MMD Loss = 0.0000\n",
      "Train Epoch: 38 Batch: 0 KLD Loss: 3.043126 \t NLL Loss: 0.960834 \t MMD Loss: 0.000039\n",
      "Train Epoch: 38 Batch: 1 KLD Loss: 2.981910 \t NLL Loss: 0.744106 \t MMD Loss: 0.000039\n",
      "Train Epoch: 38 Batch: 2 KLD Loss: 2.923179 \t NLL Loss: 0.886947 \t MMD Loss: 0.000039\n",
      "Train Epoch: 38 Batch: 3 KLD Loss: 2.865655 \t NLL Loss: 0.880350 \t MMD Loss: 0.000038\n",
      "Train Epoch: 38 Batch: 4 KLD Loss: 2.808256 \t NLL Loss: 0.841384 \t MMD Loss: 0.000038\n",
      "Train Epoch: 38 Batch: 5 KLD Loss: 2.751673 \t NLL Loss: 0.895718 \t MMD Loss: 0.000039\n",
      "====> Epoch: 38 Train set loss: KLD Loss: 2.772876 \t NLL Loss: 0.831416 \t MMD Loss: 0.000037 \t Average loss: 3.6043\n",
      "====> Test set loss: KLD Loss = 1.8966 \t NLL Loss = 0.6644 \t MMD Loss = 0.0000\n",
      "Train Epoch: 39 Batch: 0 KLD Loss: 2.696995 \t NLL Loss: 0.953794 \t MMD Loss: 0.000040\n",
      "Train Epoch: 39 Batch: 1 KLD Loss: 2.641639 \t NLL Loss: 0.742044 \t MMD Loss: 0.000040\n",
      "Train Epoch: 39 Batch: 2 KLD Loss: 2.587724 \t NLL Loss: 0.885047 \t MMD Loss: 0.000040\n",
      "Train Epoch: 39 Batch: 3 KLD Loss: 2.534784 \t NLL Loss: 0.877863 \t MMD Loss: 0.000041\n",
      "Train Epoch: 39 Batch: 4 KLD Loss: 2.482883 \t NLL Loss: 0.839646 \t MMD Loss: 0.000041\n",
      "Train Epoch: 39 Batch: 5 KLD Loss: 2.431222 \t NLL Loss: 0.894546 \t MMD Loss: 0.000041\n",
      "====> Epoch: 39 Train set loss: KLD Loss: 2.453905 \t NLL Loss: 0.828798 \t MMD Loss: 0.000039 \t Average loss: 3.2827\n",
      "====> Test set loss: KLD Loss = 1.6749 \t NLL Loss = 0.6632 \t MMD Loss = 0.0000\n",
      "Train Epoch: 40 Batch: 0 KLD Loss: 2.382225 \t NLL Loss: 0.953982 \t MMD Loss: 0.000040\n",
      "Train Epoch: 40 Batch: 1 KLD Loss: 2.333254 \t NLL Loss: 0.738702 \t MMD Loss: 0.000040\n",
      "Train Epoch: 40 Batch: 2 KLD Loss: 2.282350 \t NLL Loss: 0.881478 \t MMD Loss: 0.000041\n",
      "Train Epoch: 40 Batch: 3 KLD Loss: 2.234672 \t NLL Loss: 0.875073 \t MMD Loss: 0.000043\n",
      "Train Epoch: 40 Batch: 4 KLD Loss: 2.188065 \t NLL Loss: 0.838227 \t MMD Loss: 0.000043\n",
      "Train Epoch: 40 Batch: 5 KLD Loss: 2.140296 \t NLL Loss: 0.893269 \t MMD Loss: 0.000042\n",
      "====> Epoch: 40 Train set loss: KLD Loss: 2.164327 \t NLL Loss: 0.826850 \t MMD Loss: 0.000040 \t Average loss: 2.9911\n",
      "====> Test set loss: KLD Loss = 1.4734 \t NLL Loss = 0.6626 \t MMD Loss = 0.0000\n",
      "Train Epoch: 41 Batch: 0 KLD Loss: 2.094455 \t NLL Loss: 0.948990 \t MMD Loss: 0.000042\n",
      "Train Epoch: 41 Batch: 1 KLD Loss: 2.049358 \t NLL Loss: 0.737118 \t MMD Loss: 0.000042\n",
      "Train Epoch: 41 Batch: 2 KLD Loss: 2.004945 \t NLL Loss: 0.880516 \t MMD Loss: 0.000042\n",
      "Train Epoch: 41 Batch: 3 KLD Loss: 1.961783 \t NLL Loss: 0.873484 \t MMD Loss: 0.000043\n",
      "Train Epoch: 41 Batch: 4 KLD Loss: 1.918673 \t NLL Loss: 0.837222 \t MMD Loss: 0.000042\n",
      "Train Epoch: 41 Batch: 5 KLD Loss: 1.876182 \t NLL Loss: 0.891820 \t MMD Loss: 0.000043\n",
      "====> Epoch: 41 Train set loss: KLD Loss: 1.900113 \t NLL Loss: 0.825001 \t MMD Loss: 0.000041 \t Average loss: 2.7251\n",
      "====> Test set loss: KLD Loss = 1.2906 \t NLL Loss = 0.6617 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_41.pth\n",
      "Train Epoch: 42 Batch: 0 KLD Loss: 1.836644 \t NLL Loss: 0.947744 \t MMD Loss: 0.000044\n",
      "Train Epoch: 42 Batch: 1 KLD Loss: 1.794577 \t NLL Loss: 0.734847 \t MMD Loss: 0.000044\n",
      "Train Epoch: 42 Batch: 2 KLD Loss: 1.754061 \t NLL Loss: 0.877578 \t MMD Loss: 0.000044\n",
      "Train Epoch: 42 Batch: 3 KLD Loss: 1.714811 \t NLL Loss: 0.870954 \t MMD Loss: 0.000044\n",
      "Train Epoch: 42 Batch: 4 KLD Loss: 1.676313 \t NLL Loss: 0.835577 \t MMD Loss: 0.000044\n",
      "Train Epoch: 42 Batch: 5 KLD Loss: 1.638327 \t NLL Loss: 0.890610 \t MMD Loss: 0.000044\n",
      "====> Epoch: 42 Train set loss: KLD Loss: 1.662202 \t NLL Loss: 0.823112 \t MMD Loss: 0.000042 \t Average loss: 2.4853\n",
      "====> Test set loss: KLD Loss = 1.1261 \t NLL Loss = 0.6609 \t MMD Loss = 0.0000\n",
      "Train Epoch: 43 Batch: 0 KLD Loss: 1.600699 \t NLL Loss: 0.945722 \t MMD Loss: 0.000045\n",
      "Train Epoch: 43 Batch: 1 KLD Loss: 1.564065 \t NLL Loss: 0.733203 \t MMD Loss: 0.000045\n",
      "Train Epoch: 43 Batch: 2 KLD Loss: 1.528047 \t NLL Loss: 0.875948 \t MMD Loss: 0.000045\n",
      "Train Epoch: 43 Batch: 3 KLD Loss: 1.493053 \t NLL Loss: 0.869078 \t MMD Loss: 0.000046\n",
      "Train Epoch: 43 Batch: 4 KLD Loss: 1.458635 \t NLL Loss: 0.834461 \t MMD Loss: 0.000046\n",
      "Train Epoch: 43 Batch: 5 KLD Loss: 1.424230 \t NLL Loss: 0.889318 \t MMD Loss: 0.000046\n",
      "====> Epoch: 43 Train set loss: KLD Loss: 1.447378 \t NLL Loss: 0.821583 \t MMD Loss: 0.000043 \t Average loss: 2.2689\n",
      "====> Test set loss: KLD Loss = 0.9785 \t NLL Loss = 0.6601 \t MMD Loss = 0.0000\n",
      "Train Epoch: 44 Batch: 0 KLD Loss: 1.390868 \t NLL Loss: 0.944050 \t MMD Loss: 0.000046\n",
      "Train Epoch: 44 Batch: 1 KLD Loss: 1.358546 \t NLL Loss: 0.731470 \t MMD Loss: 0.000045\n",
      "Train Epoch: 44 Batch: 2 KLD Loss: 1.325799 \t NLL Loss: 0.873958 \t MMD Loss: 0.000046\n",
      "Train Epoch: 44 Batch: 3 KLD Loss: 1.294205 \t NLL Loss: 0.867127 \t MMD Loss: 0.000046\n",
      "Train Epoch: 44 Batch: 4 KLD Loss: 1.263313 \t NLL Loss: 0.833132 \t MMD Loss: 0.000046\n",
      "Train Epoch: 44 Batch: 5 KLD Loss: 1.232834 \t NLL Loss: 0.888051 \t MMD Loss: 0.000047\n",
      "====> Epoch: 44 Train set loss: KLD Loss: 1.255352 \t NLL Loss: 0.819996 \t MMD Loss: 0.000044 \t Average loss: 2.0753\n",
      "====> Test set loss: KLD Loss = 0.8466 \t NLL Loss = 0.6593 \t MMD Loss = 0.0000\n",
      "Train Epoch: 45 Batch: 0 KLD Loss: 1.203627 \t NLL Loss: 0.943254 \t MMD Loss: 0.000047\n",
      "Train Epoch: 45 Batch: 1 KLD Loss: 1.173863 \t NLL Loss: 0.729929 \t MMD Loss: 0.000047\n",
      "Train Epoch: 45 Batch: 2 KLD Loss: 1.145298 \t NLL Loss: 0.872074 \t MMD Loss: 0.000048\n",
      "Train Epoch: 45 Batch: 3 KLD Loss: 1.117377 \t NLL Loss: 0.865269 \t MMD Loss: 0.000047\n",
      "Train Epoch: 45 Batch: 4 KLD Loss: 1.089890 \t NLL Loss: 0.831903 \t MMD Loss: 0.000047\n",
      "Train Epoch: 45 Batch: 5 KLD Loss: 1.062912 \t NLL Loss: 0.886921 \t MMD Loss: 0.000047\n",
      "====> Epoch: 45 Train set loss: KLD Loss: 1.084164 \t NLL Loss: 0.818649 \t MMD Loss: 0.000045 \t Average loss: 1.9028\n",
      "====> Test set loss: KLD Loss = 0.7292 \t NLL Loss = 0.6586 \t MMD Loss = 0.0000\n",
      "Train Epoch: 46 Batch: 0 KLD Loss: 1.036422 \t NLL Loss: 0.941882 \t MMD Loss: 0.000048\n",
      "Train Epoch: 46 Batch: 1 KLD Loss: 1.010387 \t NLL Loss: 0.728757 \t MMD Loss: 0.000048\n",
      "Train Epoch: 46 Batch: 2 KLD Loss: 0.985132 \t NLL Loss: 0.870478 \t MMD Loss: 0.000048\n",
      "Train Epoch: 46 Batch: 3 KLD Loss: 0.960298 \t NLL Loss: 0.863341 \t MMD Loss: 0.000049\n",
      "Train Epoch: 46 Batch: 4 KLD Loss: 0.935971 \t NLL Loss: 0.830947 \t MMD Loss: 0.000049\n",
      "Train Epoch: 46 Batch: 5 KLD Loss: 0.912088 \t NLL Loss: 0.885776 \t MMD Loss: 0.000049\n",
      "====> Epoch: 46 Train set loss: KLD Loss: 0.932117 \t NLL Loss: 0.817346 \t MMD Loss: 0.000046 \t Average loss: 1.7494\n",
      "====> Test set loss: KLD Loss = 0.6256 \t NLL Loss = 0.6579 \t MMD Loss = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 Batch: 0 KLD Loss: 0.888908 \t NLL Loss: 0.941252 \t MMD Loss: 0.000049\n",
      "Train Epoch: 47 Batch: 1 KLD Loss: 0.866101 \t NLL Loss: 0.727667 \t MMD Loss: 0.000049\n",
      "Train Epoch: 47 Batch: 2 KLD Loss: 0.843526 \t NLL Loss: 0.868863 \t MMD Loss: 0.000049\n",
      "Train Epoch: 47 Batch: 3 KLD Loss: 0.821734 \t NLL Loss: 0.861617 \t MMD Loss: 0.000050\n",
      "Train Epoch: 47 Batch: 4 KLD Loss: 0.800369 \t NLL Loss: 0.829856 \t MMD Loss: 0.000050\n",
      "Train Epoch: 47 Batch: 5 KLD Loss: 0.779397 \t NLL Loss: 0.884625 \t MMD Loss: 0.000050\n",
      "====> Epoch: 47 Train set loss: KLD Loss: 0.798011 \t NLL Loss: 0.816180 \t MMD Loss: 0.000047 \t Average loss: 1.6141\n",
      "====> Test set loss: KLD Loss = 0.5342 \t NLL Loss = 0.6572 \t MMD Loss = 0.0000\n",
      "Train Epoch: 48 Batch: 0 KLD Loss: 0.759011 \t NLL Loss: 0.940294 \t MMD Loss: 0.000050\n",
      "Train Epoch: 48 Batch: 1 KLD Loss: 0.738802 \t NLL Loss: 0.726675 \t MMD Loss: 0.000050\n",
      "Train Epoch: 48 Batch: 2 KLD Loss: 0.719215 \t NLL Loss: 0.867579 \t MMD Loss: 0.000050\n",
      "Train Epoch: 48 Batch: 3 KLD Loss: 0.700153 \t NLL Loss: 0.859758 \t MMD Loss: 0.000050\n",
      "Train Epoch: 48 Batch: 4 KLD Loss: 0.681369 \t NLL Loss: 0.828949 \t MMD Loss: 0.000050\n",
      "Train Epoch: 48 Batch: 5 KLD Loss: 0.662975 \t NLL Loss: 0.883479 \t MMD Loss: 0.000050\n",
      "====> Epoch: 48 Train set loss: KLD Loss: 0.680144 \t NLL Loss: 0.815040 \t MMD Loss: 0.000048 \t Average loss: 1.4951\n",
      "====> Test set loss: KLD Loss = 0.4541 \t NLL Loss = 0.6565 \t MMD Loss = 0.0000\n",
      "Train Epoch: 49 Batch: 0 KLD Loss: 0.645373 \t NLL Loss: 0.939825 \t MMD Loss: 0.000051\n",
      "Train Epoch: 49 Batch: 1 KLD Loss: 0.627592 \t NLL Loss: 0.725619 \t MMD Loss: 0.000051\n",
      "Train Epoch: 49 Batch: 2 KLD Loss: 0.610448 \t NLL Loss: 0.866068 \t MMD Loss: 0.000051\n",
      "Train Epoch: 49 Batch: 3 KLD Loss: 0.593762 \t NLL Loss: 0.858196 \t MMD Loss: 0.000052\n",
      "Train Epoch: 49 Batch: 4 KLD Loss: 0.577425 \t NLL Loss: 0.828005 \t MMD Loss: 0.000051\n",
      "Train Epoch: 49 Batch: 5 KLD Loss: 0.561501 \t NLL Loss: 0.882355 \t MMD Loss: 0.000051\n",
      "====> Epoch: 49 Train set loss: KLD Loss: 0.577133 \t NLL Loss: 0.813976 \t MMD Loss: 0.000049 \t Average loss: 1.3911\n",
      "====> Test set loss: KLD Loss = 0.3843 \t NLL Loss = 0.6557 \t MMD Loss = 0.0000\n",
      "Train Epoch: 50 Batch: 0 KLD Loss: 0.545999 \t NLL Loss: 0.939269 \t MMD Loss: 0.000052\n",
      "Train Epoch: 50 Batch: 1 KLD Loss: 0.530733 \t NLL Loss: 0.724748 \t MMD Loss: 0.000052\n",
      "Train Epoch: 50 Batch: 2 KLD Loss: 0.515778 \t NLL Loss: 0.864998 \t MMD Loss: 0.000052\n",
      "Train Epoch: 50 Batch: 3 KLD Loss: 0.501375 \t NLL Loss: 0.856414 \t MMD Loss: 0.000053\n",
      "Train Epoch: 50 Batch: 4 KLD Loss: 0.487249 \t NLL Loss: 0.827162 \t MMD Loss: 0.000052\n",
      "Train Epoch: 50 Batch: 5 KLD Loss: 0.473345 \t NLL Loss: 0.881190 \t MMD Loss: 0.000052\n",
      "====> Epoch: 50 Train set loss: KLD Loss: 0.487498 \t NLL Loss: 0.812973 \t MMD Loss: 0.000050 \t Average loss: 1.3004\n",
      "====> Test set loss: KLD Loss = 0.3238 \t NLL Loss = 0.6551 \t MMD Loss = 0.0000\n",
      "Train Epoch: 51 Batch: 0 KLD Loss: 0.459972 \t NLL Loss: 0.939054 \t MMD Loss: 0.000053\n",
      "Train Epoch: 51 Batch: 1 KLD Loss: 0.446813 \t NLL Loss: 0.723950 \t MMD Loss: 0.000052\n",
      "Train Epoch: 51 Batch: 2 KLD Loss: 0.433861 \t NLL Loss: 0.863615 \t MMD Loss: 0.000053\n",
      "Train Epoch: 51 Batch: 3 KLD Loss: 0.421410 \t NLL Loss: 0.854881 \t MMD Loss: 0.000053\n",
      "Train Epoch: 51 Batch: 4 KLD Loss: 0.409152 \t NLL Loss: 0.826256 \t MMD Loss: 0.000053\n",
      "Train Epoch: 51 Batch: 5 KLD Loss: 0.397218 \t NLL Loss: 0.880185 \t MMD Loss: 0.000053\n",
      "====> Epoch: 51 Train set loss: KLD Loss: 0.409923 \t NLL Loss: 0.812041 \t MMD Loss: 0.000050 \t Average loss: 1.2219\n",
      "====> Test set loss: KLD Loss = 0.2717 \t NLL Loss = 0.6544 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_51.pth\n",
      "Train Epoch: 52 Batch: 0 KLD Loss: 0.386066 \t NLL Loss: 0.938947 \t MMD Loss: 0.000054\n",
      "Train Epoch: 52 Batch: 1 KLD Loss: 0.374357 \t NLL Loss: 0.723139 \t MMD Loss: 0.000054\n",
      "Train Epoch: 52 Batch: 2 KLD Loss: 0.363286 \t NLL Loss: 0.862218 \t MMD Loss: 0.000053\n",
      "Train Epoch: 52 Batch: 3 KLD Loss: 0.352657 \t NLL Loss: 0.853368 \t MMD Loss: 0.000053\n",
      "Train Epoch: 52 Batch: 4 KLD Loss: 0.342203 \t NLL Loss: 0.825526 \t MMD Loss: 0.000053\n",
      "Train Epoch: 52 Batch: 5 KLD Loss: 0.331926 \t NLL Loss: 0.879131 \t MMD Loss: 0.000053\n",
      "====> Epoch: 52 Train set loss: KLD Loss: 0.343221 \t NLL Loss: 0.811145 \t MMD Loss: 0.000051 \t Average loss: 1.1543\n",
      "====> Test set loss: KLD Loss = 0.2268 \t NLL Loss = 0.6537 \t MMD Loss = 0.0000\n",
      "Train Epoch: 53 Batch: 0 KLD Loss: 0.322054 \t NLL Loss: 0.938719 \t MMD Loss: 0.000054\n",
      "Train Epoch: 53 Batch: 1 KLD Loss: 0.312272 \t NLL Loss: 0.722452 \t MMD Loss: 0.000054\n",
      "Train Epoch: 53 Batch: 2 KLD Loss: 0.302915 \t NLL Loss: 0.861182 \t MMD Loss: 0.000054\n",
      "Train Epoch: 53 Batch: 3 KLD Loss: 0.293848 \t NLL Loss: 0.851741 \t MMD Loss: 0.000055\n",
      "Train Epoch: 53 Batch: 4 KLD Loss: 0.284784 \t NLL Loss: 0.824764 \t MMD Loss: 0.000055\n",
      "Train Epoch: 53 Batch: 5 KLD Loss: 0.276040 \t NLL Loss: 0.878050 \t MMD Loss: 0.000054\n",
      "====> Epoch: 53 Train set loss: KLD Loss: 0.285991 \t NLL Loss: 0.810280 \t MMD Loss: 0.000052 \t Average loss: 1.0962\n",
      "====> Test set loss: KLD Loss = 0.1886 \t NLL Loss = 0.6531 \t MMD Loss = 0.0000\n",
      "Train Epoch: 54 Batch: 0 KLD Loss: 0.268044 \t NLL Loss: 0.938807 \t MMD Loss: 0.000054\n",
      "Train Epoch: 54 Batch: 1 KLD Loss: 0.259731 \t NLL Loss: 0.721775 \t MMD Loss: 0.000054\n",
      "Train Epoch: 54 Batch: 2 KLD Loss: 0.251257 \t NLL Loss: 0.859985 \t MMD Loss: 0.000055\n",
      "Train Epoch: 54 Batch: 3 KLD Loss: 0.243582 \t NLL Loss: 0.850323 \t MMD Loss: 0.000055\n",
      "Train Epoch: 54 Batch: 4 KLD Loss: 0.236120 \t NLL Loss: 0.824007 \t MMD Loss: 0.000055\n",
      "Train Epoch: 54 Batch: 5 KLD Loss: 0.228544 \t NLL Loss: 0.876996 \t MMD Loss: 0.000055\n",
      "====> Epoch: 54 Train set loss: KLD Loss: 0.237371 \t NLL Loss: 0.809479 \t MMD Loss: 0.000052 \t Average loss: 1.0468\n",
      "====> Test set loss: KLD Loss = 0.1560 \t NLL Loss = 0.6525 \t MMD Loss = 0.0000\n",
      "Train Epoch: 55 Batch: 0 KLD Loss: 0.221684 \t NLL Loss: 0.938784 \t MMD Loss: 0.000055\n",
      "Train Epoch: 55 Batch: 1 KLD Loss: 0.214368 \t NLL Loss: 0.721082 \t MMD Loss: 0.000055\n",
      "Train Epoch: 55 Batch: 2 KLD Loss: 0.207791 \t NLL Loss: 0.859073 \t MMD Loss: 0.000055\n",
      "Train Epoch: 55 Batch: 3 KLD Loss: 0.201651 \t NLL Loss: 0.848951 \t MMD Loss: 0.000055\n",
      "Train Epoch: 55 Batch: 4 KLD Loss: 0.194973 \t NLL Loss: 0.823278 \t MMD Loss: 0.000055\n",
      "Train Epoch: 55 Batch: 5 KLD Loss: 0.188366 \t NLL Loss: 0.875868 \t MMD Loss: 0.000055\n",
      "====> Epoch: 55 Train set loss: KLD Loss: 0.196123 \t NLL Loss: 0.808704 \t MMD Loss: 0.000053 \t Average loss: 1.0048\n",
      "====> Test set loss: KLD Loss = 0.1288 \t NLL Loss = 0.6518 \t MMD Loss = 0.0000\n",
      "Train Epoch: 56 Batch: 0 KLD Loss: 0.183427 \t NLL Loss: 0.939019 \t MMD Loss: 0.000056\n",
      "Train Epoch: 56 Batch: 1 KLD Loss: 0.177032 \t NLL Loss: 0.720421 \t MMD Loss: 0.000057\n",
      "Train Epoch: 56 Batch: 2 KLD Loss: 0.170858 \t NLL Loss: 0.857898 \t MMD Loss: 0.000056\n",
      "Train Epoch: 56 Batch: 3 KLD Loss: 0.165234 \t NLL Loss: 0.847355 \t MMD Loss: 0.000056\n",
      "Train Epoch: 56 Batch: 4 KLD Loss: 0.160069 \t NLL Loss: 0.822516 \t MMD Loss: 0.000056\n",
      "Train Epoch: 56 Batch: 5 KLD Loss: 0.154928 \t NLL Loss: 0.874839 \t MMD Loss: 0.000055\n",
      "====> Epoch: 56 Train set loss: KLD Loss: 0.161444 \t NLL Loss: 0.807908 \t MMD Loss: 0.000054 \t Average loss: 0.9693\n",
      "====> Test set loss: KLD Loss = 0.1056 \t NLL Loss = 0.6512 \t MMD Loss = 0.0000\n",
      "Train Epoch: 57 Batch: 0 KLD Loss: 0.150679 \t NLL Loss: 0.939001 \t MMD Loss: 0.000056\n",
      "Train Epoch: 57 Batch: 1 KLD Loss: 0.144889 \t NLL Loss: 0.719828 \t MMD Loss: 0.000056\n",
      "Train Epoch: 57 Batch: 2 KLD Loss: 0.140330 \t NLL Loss: 0.856934 \t MMD Loss: 0.000057\n",
      "Train Epoch: 57 Batch: 3 KLD Loss: 0.137336 \t NLL Loss: 0.845957 \t MMD Loss: 0.000058\n",
      "Train Epoch: 57 Batch: 4 KLD Loss: 0.131967 \t NLL Loss: 0.822027 \t MMD Loss: 0.000058\n",
      "Train Epoch: 57 Batch: 5 KLD Loss: 0.126325 \t NLL Loss: 0.873786 \t MMD Loss: 0.000056\n",
      "====> Epoch: 57 Train set loss: KLD Loss: 0.132712 \t NLL Loss: 0.807188 \t MMD Loss: 0.000054 \t Average loss: 0.9398\n",
      "====> Test set loss: KLD Loss = 0.0865 \t NLL Loss = 0.6506 \t MMD Loss = 0.0000\n",
      "Train Epoch: 58 Batch: 0 KLD Loss: 0.123786 \t NLL Loss: 0.939321 \t MMD Loss: 0.000056\n",
      "Train Epoch: 58 Batch: 1 KLD Loss: 0.119591 \t NLL Loss: 0.719180 \t MMD Loss: 0.000056\n",
      "Train Epoch: 58 Batch: 2 KLD Loss: 0.114615 \t NLL Loss: 0.855941 \t MMD Loss: 0.000056\n",
      "Train Epoch: 58 Batch: 3 KLD Loss: 0.110342 \t NLL Loss: 0.844574 \t MMD Loss: 0.000057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 Batch: 4 KLD Loss: 0.106506 \t NLL Loss: 0.821373 \t MMD Loss: 0.000057\n",
      "Train Epoch: 58 Batch: 5 KLD Loss: 0.103187 \t NLL Loss: 0.872757 \t MMD Loss: 0.000057\n",
      "====> Epoch: 58 Train set loss: KLD Loss: 0.108214 \t NLL Loss: 0.806487 \t MMD Loss: 0.000054 \t Average loss: 0.9146\n",
      "====> Test set loss: KLD Loss = 0.0705 \t NLL Loss = 0.6499 \t MMD Loss = 0.0000\n",
      "Train Epoch: 59 Batch: 0 KLD Loss: 0.104142 \t NLL Loss: 0.939722 \t MMD Loss: 0.000059\n",
      "Train Epoch: 59 Batch: 1 KLD Loss: 0.098063 \t NLL Loss: 0.718555 \t MMD Loss: 0.000059\n",
      "Train Epoch: 59 Batch: 2 KLD Loss: 0.093085 \t NLL Loss: 0.855033 \t MMD Loss: 0.000057\n",
      "Train Epoch: 59 Batch: 3 KLD Loss: 0.093495 \t NLL Loss: 0.843224 \t MMD Loss: 0.000056\n",
      "Train Epoch: 59 Batch: 4 KLD Loss: 0.091079 \t NLL Loss: 0.820722 \t MMD Loss: 0.000055\n",
      "Train Epoch: 59 Batch: 5 KLD Loss: 0.084528 \t NLL Loss: 0.871667 \t MMD Loss: 0.000056\n",
      "====> Epoch: 59 Train set loss: KLD Loss: 0.090077 \t NLL Loss: 0.805813 \t MMD Loss: 0.000055 \t Average loss: 0.8958\n",
      "====> Test set loss: KLD Loss = 0.0572 \t NLL Loss = 0.6493 \t MMD Loss = 0.0000\n",
      "Train Epoch: 60 Batch: 0 KLD Loss: 0.080856 \t NLL Loss: 0.939861 \t MMD Loss: 0.000058\n",
      "Train Epoch: 60 Batch: 1 KLD Loss: 0.079439 \t NLL Loss: 0.718064 \t MMD Loss: 0.000059\n",
      "Train Epoch: 60 Batch: 2 KLD Loss: 0.077988 \t NLL Loss: 0.854282 \t MMD Loss: 0.000059\n",
      "Train Epoch: 60 Batch: 3 KLD Loss: 0.075228 \t NLL Loss: 0.841834 \t MMD Loss: 0.000059\n",
      "Train Epoch: 60 Batch: 4 KLD Loss: 0.070170 \t NLL Loss: 0.819963 \t MMD Loss: 0.000059\n",
      "Train Epoch: 60 Batch: 5 KLD Loss: 0.068391 \t NLL Loss: 0.870514 \t MMD Loss: 0.000057\n",
      "====> Epoch: 60 Train set loss: KLD Loss: 0.072151 \t NLL Loss: 0.805110 \t MMD Loss: 0.000056 \t Average loss: 0.8772\n",
      "====> Test set loss: KLD Loss = 0.0474 \t NLL Loss = 0.6486 \t MMD Loss = 0.0000\n",
      "Train Epoch: 61 Batch: 0 KLD Loss: 0.079094 \t NLL Loss: 0.940242 \t MMD Loss: 0.000055\n",
      "Train Epoch: 61 Batch: 1 KLD Loss: 0.070422 \t NLL Loss: 0.717698 \t MMD Loss: 0.000055\n",
      "Train Epoch: 61 Batch: 2 KLD Loss: 0.060497 \t NLL Loss: 0.853151 \t MMD Loss: 0.000058\n",
      "Train Epoch: 61 Batch: 3 KLD Loss: 0.062787 \t NLL Loss: 0.840455 \t MMD Loss: 0.000060\n",
      "Train Epoch: 61 Batch: 4 KLD Loss: 0.063875 \t NLL Loss: 0.819464 \t MMD Loss: 0.000061\n",
      "Train Epoch: 61 Batch: 5 KLD Loss: 0.056161 \t NLL Loss: 0.869539 \t MMD Loss: 0.000059\n",
      "====> Epoch: 61 Train set loss: KLD Loss: 0.062697 \t NLL Loss: 0.804477 \t MMD Loss: 0.000056 \t Average loss: 0.8671\n",
      "====> Test set loss: KLD Loss = 0.0369 \t NLL Loss = 0.6480 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_61.pth\n",
      "Train Epoch: 62 Batch: 0 KLD Loss: 0.053037 \t NLL Loss: 0.940901 \t MMD Loss: 0.000059\n",
      "Train Epoch: 62 Batch: 1 KLD Loss: 0.050541 \t NLL Loss: 0.716921 \t MMD Loss: 0.000058\n",
      "Train Epoch: 62 Batch: 2 KLD Loss: 0.052332 \t NLL Loss: 0.852676 \t MMD Loss: 0.000057\n",
      "Train Epoch: 62 Batch: 3 KLD Loss: 0.057945 \t NLL Loss: 0.839313 \t MMD Loss: 0.000055\n",
      "Train Epoch: 62 Batch: 4 KLD Loss: 0.052442 \t NLL Loss: 0.818864 \t MMD Loss: 0.000055\n",
      "Train Epoch: 62 Batch: 5 KLD Loss: 0.044629 \t NLL Loss: 0.868540 \t MMD Loss: 0.000057\n",
      "====> Epoch: 62 Train set loss: KLD Loss: 0.049624 \t NLL Loss: 0.803945 \t MMD Loss: 0.000055 \t Average loss: 0.8535\n",
      "====> Test set loss: KLD Loss = 0.0307 \t NLL Loss = 0.6474 \t MMD Loss = 0.0000\n",
      "Train Epoch: 63 Batch: 0 KLD Loss: 0.058349 \t NLL Loss: 0.941481 \t MMD Loss: 0.000062\n",
      "Train Epoch: 63 Batch: 1 KLD Loss: 0.056523 \t NLL Loss: 0.716282 \t MMD Loss: 0.000063\n",
      "Train Epoch: 63 Batch: 2 KLD Loss: 0.042307 \t NLL Loss: 0.851630 \t MMD Loss: 0.000060\n",
      "Train Epoch: 63 Batch: 3 KLD Loss: 0.038376 \t NLL Loss: 0.837903 \t MMD Loss: 0.000058\n",
      "Train Epoch: 63 Batch: 4 KLD Loss: 0.041301 \t NLL Loss: 0.818267 \t MMD Loss: 0.000056\n",
      "Train Epoch: 63 Batch: 5 KLD Loss: 0.040336 \t NLL Loss: 0.867477 \t MMD Loss: 0.000056\n",
      "====> Epoch: 63 Train set loss: KLD Loss: 0.044240 \t NLL Loss: 0.803278 \t MMD Loss: 0.000057 \t Average loss: 0.8475\n",
      "====> Test set loss: KLD Loss = 0.0246 \t NLL Loss = 0.6467 \t MMD Loss = 0.0000\n",
      "Train Epoch: 64 Batch: 0 KLD Loss: 0.043031 \t NLL Loss: 0.941467 \t MMD Loss: 0.000057\n",
      "Train Epoch: 64 Batch: 1 KLD Loss: 0.033715 \t NLL Loss: 0.715988 \t MMD Loss: 0.000057\n",
      "Train Epoch: 64 Batch: 2 KLD Loss: 0.034910 \t NLL Loss: 0.850769 \t MMD Loss: 0.000060\n",
      "Train Epoch: 64 Batch: 3 KLD Loss: 0.045970 \t NLL Loss: 0.836592 \t MMD Loss: 0.000063\n",
      "Train Epoch: 64 Batch: 4 KLD Loss: 0.041927 \t NLL Loss: 0.817824 \t MMD Loss: 0.000063\n",
      "Train Epoch: 64 Batch: 5 KLD Loss: 0.029078 \t NLL Loss: 0.866443 \t MMD Loss: 0.000060\n",
      "====> Epoch: 64 Train set loss: KLD Loss: 0.036490 \t NLL Loss: 0.802647 \t MMD Loss: 0.000057 \t Average loss: 0.8391\n",
      "====> Test set loss: KLD Loss = 0.0196 \t NLL Loss = 0.6461 \t MMD Loss = 0.0000\n",
      "Train Epoch: 65 Batch: 0 KLD Loss: 0.038993 \t NLL Loss: 0.941798 \t MMD Loss: 0.000057\n",
      "Train Epoch: 65 Batch: 1 KLD Loss: 0.040229 \t NLL Loss: 0.715574 \t MMD Loss: 0.000055\n",
      "Train Epoch: 65 Batch: 2 KLD Loss: 0.029186 \t NLL Loss: 0.850346 \t MMD Loss: 0.000057\n",
      "Train Epoch: 65 Batch: 3 KLD Loss: 0.024015 \t NLL Loss: 0.835350 \t MMD Loss: 0.000059\n",
      "Train Epoch: 65 Batch: 4 KLD Loss: 0.023901 \t NLL Loss: 0.817303 \t MMD Loss: 0.000060\n",
      "Train Epoch: 65 Batch: 5 KLD Loss: 0.025029 \t NLL Loss: 0.865492 \t MMD Loss: 0.000060\n",
      "====> Epoch: 65 Train set loss: KLD Loss: 0.028944 \t NLL Loss: 0.802133 \t MMD Loss: 0.000055 \t Average loss: 0.8310\n",
      "====> Test set loss: KLD Loss = 0.0167 \t NLL Loss = 0.6455 \t MMD Loss = 0.0000\n",
      "Train Epoch: 66 Batch: 0 KLD Loss: 0.038339 \t NLL Loss: 0.943848 \t MMD Loss: 0.000062\n",
      "Train Epoch: 66 Batch: 1 KLD Loss: 0.026953 \t NLL Loss: 0.714906 \t MMD Loss: 0.000062\n",
      "Train Epoch: 66 Batch: 2 KLD Loss: 0.020847 \t NLL Loss: 0.849519 \t MMD Loss: 0.000059\n",
      "Train Epoch: 66 Batch: 3 KLD Loss: 0.033877 \t NLL Loss: 0.834336 \t MMD Loss: 0.000056\n",
      "Train Epoch: 66 Batch: 4 KLD Loss: 0.036941 \t NLL Loss: 0.816851 \t MMD Loss: 0.000055\n",
      "Train Epoch: 66 Batch: 5 KLD Loss: 0.024787 \t NLL Loss: 0.864724 \t MMD Loss: 0.000057\n",
      "====> Epoch: 66 Train set loss: KLD Loss: 0.029007 \t NLL Loss: 0.801865 \t MMD Loss: 0.000056 \t Average loss: 0.8308\n",
      "====> Test set loss: KLD Loss = 0.0118 \t NLL Loss = 0.6451 \t MMD Loss = 0.0000\n",
      "Train Epoch: 67 Batch: 0 KLD Loss: 0.018733 \t NLL Loss: 0.942953 \t MMD Loss: 0.000061\n",
      "Train Epoch: 67 Batch: 1 KLD Loss: 0.023534 \t NLL Loss: 0.714409 \t MMD Loss: 0.000062\n",
      "Train Epoch: 67 Batch: 2 KLD Loss: 0.023559 \t NLL Loss: 0.848887 \t MMD Loss: 0.000062\n",
      "Train Epoch: 67 Batch: 3 KLD Loss: 0.018765 \t NLL Loss: 0.832870 \t MMD Loss: 0.000061\n",
      "Train Epoch: 67 Batch: 4 KLD Loss: 0.014700 \t NLL Loss: 0.816284 \t MMD Loss: 0.000060\n",
      "Train Epoch: 67 Batch: 5 KLD Loss: 0.014594 \t NLL Loss: 0.863426 \t MMD Loss: 0.000059\n",
      "====> Epoch: 67 Train set loss: KLD Loss: 0.018176 \t NLL Loss: 0.801010 \t MMD Loss: 0.000058 \t Average loss: 0.8191\n",
      "====> Test set loss: KLD Loss = 0.0113 \t NLL Loss = 0.6442 \t MMD Loss = 0.0000\n",
      "Train Epoch: 68 Batch: 0 KLD Loss: 0.033879 \t NLL Loss: 0.943881 \t MMD Loss: 0.000056\n",
      "Train Epoch: 68 Batch: 1 KLD Loss: 0.026107 \t NLL Loss: 0.714179 \t MMD Loss: 0.000056\n",
      "Train Epoch: 68 Batch: 2 KLD Loss: 0.012857 \t NLL Loss: 0.847995 \t MMD Loss: 0.000059\n",
      "Train Epoch: 68 Batch: 3 KLD Loss: 0.018095 \t NLL Loss: 0.831622 \t MMD Loss: 0.000062\n",
      "Train Epoch: 68 Batch: 4 KLD Loss: 0.024756 \t NLL Loss: 0.815678 \t MMD Loss: 0.000063\n",
      "Train Epoch: 68 Batch: 5 KLD Loss: 0.016638 \t NLL Loss: 0.862525 \t MMD Loss: 0.000062\n",
      "====> Epoch: 68 Train set loss: KLD Loss: 0.021120 \t NLL Loss: 0.800539 \t MMD Loss: 0.000057 \t Average loss: 0.8216\n",
      "====> Test set loss: KLD Loss = 0.0079 \t NLL Loss = 0.6438 \t MMD Loss = 0.0000\n",
      "Train Epoch: 69 Batch: 0 KLD Loss: 0.010385 \t NLL Loss: 0.943837 \t MMD Loss: 0.000060\n",
      "Train Epoch: 69 Batch: 1 KLD Loss: 0.011391 \t NLL Loss: 0.713673 \t MMD Loss: 0.000059\n",
      "Train Epoch: 69 Batch: 2 KLD Loss: 0.013435 \t NLL Loss: 0.847987 \t MMD Loss: 0.000058\n",
      "Train Epoch: 69 Batch: 3 KLD Loss: 0.016516 \t NLL Loss: 0.830578 \t MMD Loss: 0.000057\n",
      "Train Epoch: 69 Batch: 4 KLD Loss: 0.012369 \t NLL Loss: 0.815245 \t MMD Loss: 0.000057\n",
      "Train Epoch: 69 Batch: 5 KLD Loss: 0.008792 \t NLL Loss: 0.861512 \t MMD Loss: 0.000059\n",
      "====> Epoch: 69 Train set loss: KLD Loss: 0.011633 \t NLL Loss: 0.800053 \t MMD Loss: 0.000056 \t Average loss: 0.8116\n",
      "====> Test set loss: KLD Loss = 0.0067 \t NLL Loss = 0.6430 \t MMD Loss = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 70 Batch: 0 KLD Loss: 0.022416 \t NLL Loss: 0.945748 \t MMD Loss: 0.000063\n",
      "Train Epoch: 70 Batch: 1 KLD Loss: 0.019717 \t NLL Loss: 0.713291 \t MMD Loss: 0.000064\n",
      "Train Epoch: 70 Batch: 2 KLD Loss: 0.009653 \t NLL Loss: 0.846787 \t MMD Loss: 0.000061\n",
      "Train Epoch: 70 Batch: 3 KLD Loss: 0.009097 \t NLL Loss: 0.829514 \t MMD Loss: 0.000059\n",
      "Train Epoch: 70 Batch: 4 KLD Loss: 0.012992 \t NLL Loss: 0.814832 \t MMD Loss: 0.000057\n",
      "Train Epoch: 70 Batch: 5 KLD Loss: 0.011830 \t NLL Loss: 0.860675 \t MMD Loss: 0.000058\n",
      "====> Epoch: 70 Train set loss: KLD Loss: 0.013679 \t NLL Loss: 0.799736 \t MMD Loss: 0.000058 \t Average loss: 0.8134\n",
      "====> Test set loss: KLD Loss = 0.0051 \t NLL Loss = 0.6426 \t MMD Loss = 0.0000\n",
      "Train Epoch: 71 Batch: 0 KLD Loss: 0.008385 \t NLL Loss: 0.944848 \t MMD Loss: 0.000059\n",
      "Train Epoch: 71 Batch: 1 KLD Loss: 0.006022 \t NLL Loss: 0.712709 \t MMD Loss: 0.000060\n",
      "Train Epoch: 71 Batch: 2 KLD Loss: 0.008511 \t NLL Loss: 0.846378 \t MMD Loss: 0.000061\n",
      "Train Epoch: 71 Batch: 3 KLD Loss: 0.012981 \t NLL Loss: 0.828184 \t MMD Loss: 0.000063\n",
      "Train Epoch: 71 Batch: 4 KLD Loss: 0.011631 \t NLL Loss: 0.814454 \t MMD Loss: 0.000063\n",
      "Train Epoch: 71 Batch: 5 KLD Loss: 0.005821 \t NLL Loss: 0.859577 \t MMD Loss: 0.000061\n",
      "====> Epoch: 71 Train set loss: KLD Loss: 0.008515 \t NLL Loss: 0.798987 \t MMD Loss: 0.000058 \t Average loss: 0.8074\n",
      "====> Test set loss: KLD Loss = 0.0042 \t NLL Loss = 0.6418 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_71.pth\n",
      "Train Epoch: 72 Batch: 0 KLD Loss: 0.013168 \t NLL Loss: 0.945908 \t MMD Loss: 0.000058\n",
      "Train Epoch: 72 Batch: 1 KLD Loss: 0.014182 \t NLL Loss: 0.712414 \t MMD Loss: 0.000057\n",
      "Train Epoch: 72 Batch: 2 KLD Loss: 0.006705 \t NLL Loss: 0.845794 \t MMD Loss: 0.000058\n",
      "Train Epoch: 72 Batch: 3 KLD Loss: 0.004255 \t NLL Loss: 0.827025 \t MMD Loss: 0.000060\n",
      "Train Epoch: 72 Batch: 4 KLD Loss: 0.006110 \t NLL Loss: 0.813818 \t MMD Loss: 0.000061\n",
      "Train Epoch: 72 Batch: 5 KLD Loss: 0.006273 \t NLL Loss: 0.858626 \t MMD Loss: 0.000061\n",
      "====> Epoch: 72 Train set loss: KLD Loss: 0.008091 \t NLL Loss: 0.798577 \t MMD Loss: 0.000057 \t Average loss: 0.8066\n",
      "====> Test set loss: KLD Loss = 0.0038 \t NLL Loss = 0.6414 \t MMD Loss = 0.0000\n",
      "Train Epoch: 73 Batch: 0 KLD Loss: 0.008422 \t NLL Loss: 0.946347 \t MMD Loss: 0.000062\n",
      "Train Epoch: 73 Batch: 1 KLD Loss: 0.004773 \t NLL Loss: 0.711714 \t MMD Loss: 0.000062\n",
      "Train Epoch: 73 Batch: 2 KLD Loss: 0.004296 \t NLL Loss: 0.845160 \t MMD Loss: 0.000060\n",
      "Train Epoch: 73 Batch: 3 KLD Loss: 0.009867 \t NLL Loss: 0.825886 \t MMD Loss: 0.000058\n",
      "Train Epoch: 73 Batch: 4 KLD Loss: 0.010072 \t NLL Loss: 0.813453 \t MMD Loss: 0.000057\n",
      "Train Epoch: 73 Batch: 5 KLD Loss: 0.005188 \t NLL Loss: 0.857698 \t MMD Loss: 0.000058\n",
      "====> Epoch: 73 Train set loss: KLD Loss: 0.006802 \t NLL Loss: 0.798046 \t MMD Loss: 0.000057 \t Average loss: 0.8048\n",
      "====> Test set loss: KLD Loss = 0.0023 \t NLL Loss = 0.6408 \t MMD Loss = 0.0000\n",
      "Train Epoch: 74 Batch: 0 KLD Loss: 0.005823 \t NLL Loss: 0.946994 \t MMD Loss: 0.000061\n",
      "Train Epoch: 74 Batch: 1 KLD Loss: 0.008064 \t NLL Loss: 0.711323 \t MMD Loss: 0.000063\n",
      "Train Epoch: 74 Batch: 2 KLD Loss: 0.005791 \t NLL Loss: 0.844606 \t MMD Loss: 0.000062\n",
      "Train Epoch: 74 Batch: 3 KLD Loss: 0.002818 \t NLL Loss: 0.824853 \t MMD Loss: 0.000061\n",
      "Train Epoch: 74 Batch: 4 KLD Loss: 0.002652 \t NLL Loss: 0.812960 \t MMD Loss: 0.000060\n",
      "Train Epoch: 74 Batch: 5 KLD Loss: 0.003606 \t NLL Loss: 0.856706 \t MMD Loss: 0.000059\n",
      "====> Epoch: 74 Train set loss: KLD Loss: 0.004589 \t NLL Loss: 0.797597 \t MMD Loss: 0.000058 \t Average loss: 0.8021\n",
      "====> Test set loss: KLD Loss = 0.0026 \t NLL Loss = 0.6401 \t MMD Loss = 0.0000\n",
      "Train Epoch: 75 Batch: 0 KLD Loss: 0.008651 \t NLL Loss: 0.947240 \t MMD Loss: 0.000058\n",
      "Train Epoch: 75 Batch: 1 KLD Loss: 0.005054 \t NLL Loss: 0.711029 \t MMD Loss: 0.000059\n",
      "Train Epoch: 75 Batch: 2 KLD Loss: 0.002421 \t NLL Loss: 0.843967 \t MMD Loss: 0.000060\n",
      "Train Epoch: 75 Batch: 3 KLD Loss: 0.006245 \t NLL Loss: 0.823705 \t MMD Loss: 0.000062\n",
      "Train Epoch: 75 Batch: 4 KLD Loss: 0.007916 \t NLL Loss: 0.812605 \t MMD Loss: 0.000063\n",
      "Train Epoch: 75 Batch: 5 KLD Loss: 0.003469 \t NLL Loss: 0.855741 \t MMD Loss: 0.000061\n",
      "====> Epoch: 75 Train set loss: KLD Loss: 0.005388 \t NLL Loss: 0.797093 \t MMD Loss: 0.000058 \t Average loss: 0.8024\n",
      "====> Test set loss: KLD Loss = 0.0016 \t NLL Loss = 0.6396 \t MMD Loss = 0.0000\n",
      "Train Epoch: 76 Batch: 0 KLD Loss: 0.002738 \t NLL Loss: 0.947875 \t MMD Loss: 0.000060\n",
      "Train Epoch: 76 Batch: 1 KLD Loss: 0.004171 \t NLL Loss: 0.710672 \t MMD Loss: 0.000059\n",
      "Train Epoch: 76 Batch: 2 KLD Loss: 0.003489 \t NLL Loss: 0.843644 \t MMD Loss: 0.000059\n",
      "Train Epoch: 76 Batch: 3 KLD Loss: 0.002815 \t NLL Loss: 0.822623 \t MMD Loss: 0.000059\n",
      "Train Epoch: 76 Batch: 4 KLD Loss: 0.001656 \t NLL Loss: 0.812077 \t MMD Loss: 0.000060\n",
      "Train Epoch: 76 Batch: 5 KLD Loss: 0.001702 \t NLL Loss: 0.854852 \t MMD Loss: 0.000060\n",
      "====> Epoch: 76 Train set loss: KLD Loss: 0.002645 \t NLL Loss: 0.796687 \t MMD Loss: 0.000057 \t Average loss: 0.7993\n",
      "====> Test set loss: KLD Loss = 0.0019 \t NLL Loss = 0.6390 \t MMD Loss = 0.0000\n",
      "Train Epoch: 77 Batch: 0 KLD Loss: 0.007828 \t NLL Loss: 0.948819 \t MMD Loss: 0.000063\n",
      "Train Epoch: 77 Batch: 1 KLD Loss: 0.005092 \t NLL Loss: 0.710253 \t MMD Loss: 0.000063\n",
      "Train Epoch: 77 Batch: 2 KLD Loss: 0.001702 \t NLL Loss: 0.843095 \t MMD Loss: 0.000061\n",
      "Train Epoch: 77 Batch: 3 KLD Loss: 0.004188 \t NLL Loss: 0.821511 \t MMD Loss: 0.000059\n",
      "Train Epoch: 77 Batch: 4 KLD Loss: 0.005874 \t NLL Loss: 0.811685 \t MMD Loss: 0.000058\n",
      "Train Epoch: 77 Batch: 5 KLD Loss: 0.003554 \t NLL Loss: 0.853963 \t MMD Loss: 0.000059\n",
      "====> Epoch: 77 Train set loss: KLD Loss: 0.004507 \t NLL Loss: 0.796301 \t MMD Loss: 0.000058 \t Average loss: 0.8008\n",
      "====> Test set loss: KLD Loss = 0.0010 \t NLL Loss = 0.6384 \t MMD Loss = 0.0000\n",
      "Train Epoch: 78 Batch: 0 KLD Loss: 0.001125 \t NLL Loss: 0.948969 \t MMD Loss: 0.000061\n",
      "Train Epoch: 78 Batch: 1 KLD Loss: 0.001900 \t NLL Loss: 0.709895 \t MMD Loss: 0.000061\n",
      "Train Epoch: 78 Batch: 2 KLD Loss: 0.003250 \t NLL Loss: 0.842559 \t MMD Loss: 0.000062\n",
      "Train Epoch: 78 Batch: 3 KLD Loss: 0.003246 \t NLL Loss: 0.820450 \t MMD Loss: 0.000062\n",
      "Train Epoch: 78 Batch: 4 KLD Loss: 0.001843 \t NLL Loss: 0.811278 \t MMD Loss: 0.000061\n",
      "Train Epoch: 78 Batch: 5 KLD Loss: 0.001028 \t NLL Loss: 0.852933 \t MMD Loss: 0.000060\n",
      "====> Epoch: 78 Train set loss: KLD Loss: 0.001978 \t NLL Loss: 0.795784 \t MMD Loss: 0.000059 \t Average loss: 0.7977\n",
      "====> Test set loss: KLD Loss = 0.0014 \t NLL Loss = 0.6377 \t MMD Loss = 0.0000\n",
      "Train Epoch: 79 Batch: 0 KLD Loss: 0.007775 \t NLL Loss: 0.949541 \t MMD Loss: 0.000059\n",
      "Train Epoch: 79 Batch: 1 KLD Loss: 0.005653 \t NLL Loss: 0.709725 \t MMD Loss: 0.000058\n",
      "Train Epoch: 79 Batch: 2 KLD Loss: 0.001043 \t NLL Loss: 0.841972 \t MMD Loss: 0.000060\n",
      "Train Epoch: 79 Batch: 3 KLD Loss: 0.002220 \t NLL Loss: 0.819315 \t MMD Loss: 0.000062\n",
      "Train Epoch: 79 Batch: 4 KLD Loss: 0.004355 \t NLL Loss: 0.810782 \t MMD Loss: 0.000062\n",
      "Train Epoch: 79 Batch: 5 KLD Loss: 0.002306 \t NLL Loss: 0.852009 \t MMD Loss: 0.000062\n",
      "====> Epoch: 79 Train set loss: KLD Loss: 0.003727 \t NLL Loss: 0.795347 \t MMD Loss: 0.000058 \t Average loss: 0.7990\n",
      "====> Test set loss: KLD Loss = 0.0009 \t NLL Loss = 0.6372 \t MMD Loss = 0.0000\n",
      "Train Epoch: 80 Batch: 0 KLD Loss: 0.001016 \t NLL Loss: 0.950039 \t MMD Loss: 0.000061\n",
      "Train Epoch: 80 Batch: 1 KLD Loss: 0.000803 \t NLL Loss: 0.709259 \t MMD Loss: 0.000060\n",
      "Train Epoch: 80 Batch: 2 KLD Loss: 0.001982 \t NLL Loss: 0.841754 \t MMD Loss: 0.000060\n",
      "Train Epoch: 80 Batch: 3 KLD Loss: 0.004273 \t NLL Loss: 0.818352 \t MMD Loss: 0.000059\n",
      "Train Epoch: 80 Batch: 4 KLD Loss: 0.002673 \t NLL Loss: 0.810440 \t MMD Loss: 0.000059\n",
      "Train Epoch: 80 Batch: 5 KLD Loss: 0.000773 \t NLL Loss: 0.851073 \t MMD Loss: 0.000060\n",
      "====> Epoch: 80 Train set loss: KLD Loss: 0.001838 \t NLL Loss: 0.794959 \t MMD Loss: 0.000057 \t Average loss: 0.7967\n",
      "====> Test set loss: KLD Loss = 0.0011 \t NLL Loss = 0.6366 \t MMD Loss = 0.0000\n",
      "Train Epoch: 81 Batch: 0 KLD Loss: 0.006479 \t NLL Loss: 0.950993 \t MMD Loss: 0.000063\n",
      "Train Epoch: 81 Batch: 1 KLD Loss: 0.005673 \t NLL Loss: 0.708829 \t MMD Loss: 0.000063\n",
      "Train Epoch: 81 Batch: 2 KLD Loss: 0.001538 \t NLL Loss: 0.841055 \t MMD Loss: 0.000061\n",
      "Train Epoch: 81 Batch: 3 KLD Loss: 0.001206 \t NLL Loss: 0.817359 \t MMD Loss: 0.000060\n",
      "Train Epoch: 81 Batch: 4 KLD Loss: 0.002914 \t NLL Loss: 0.809963 \t MMD Loss: 0.000059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 Batch: 5 KLD Loss: 0.002607 \t NLL Loss: 0.850212 \t MMD Loss: 0.000059\n",
      "====> Epoch: 81 Train set loss: KLD Loss: 0.003259 \t NLL Loss: 0.794560 \t MMD Loss: 0.000058 \t Average loss: 0.7978\n",
      "====> Test set loss: KLD Loss = 0.0008 \t NLL Loss = 0.6361 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_81.pth\n",
      "Train Epoch: 82 Batch: 0 KLD Loss: 0.002323 \t NLL Loss: 0.951113 \t MMD Loss: 0.000060\n",
      "Train Epoch: 82 Batch: 1 KLD Loss: 0.000548 \t NLL Loss: 0.708559 \t MMD Loss: 0.000060\n",
      "Train Epoch: 82 Batch: 2 KLD Loss: 0.002197 \t NLL Loss: 0.840438 \t MMD Loss: 0.000062\n",
      "Train Epoch: 82 Batch: 3 KLD Loss: 0.005367 \t NLL Loss: 0.816325 \t MMD Loss: 0.000063\n",
      "Train Epoch: 82 Batch: 4 KLD Loss: 0.004191 \t NLL Loss: 0.809638 \t MMD Loss: 0.000063\n",
      "Train Epoch: 82 Batch: 5 KLD Loss: 0.000702 \t NLL Loss: 0.849240 \t MMD Loss: 0.000061\n",
      "====> Epoch: 82 Train set loss: KLD Loss: 0.002446 \t NLL Loss: 0.794065 \t MMD Loss: 0.000059 \t Average loss: 0.7965\n",
      "====> Test set loss: KLD Loss = 0.0011 \t NLL Loss = 0.6355 \t MMD Loss = 0.0000\n",
      "Train Epoch: 83 Batch: 0 KLD Loss: 0.006481 \t NLL Loss: 0.951581 \t MMD Loss: 0.000059\n",
      "Train Epoch: 83 Batch: 1 KLD Loss: 0.006326 \t NLL Loss: 0.708290 \t MMD Loss: 0.000058\n",
      "Train Epoch: 83 Batch: 2 KLD Loss: 0.001367 \t NLL Loss: 0.840214 \t MMD Loss: 0.000060\n",
      "Train Epoch: 83 Batch: 3 KLD Loss: 0.000455 \t NLL Loss: 0.815272 \t MMD Loss: 0.000061\n",
      "Train Epoch: 83 Batch: 4 KLD Loss: 0.001881 \t NLL Loss: 0.809226 \t MMD Loss: 0.000062\n",
      "Train Epoch: 83 Batch: 5 KLD Loss: 0.001958 \t NLL Loss: 0.848359 \t MMD Loss: 0.000062\n",
      "====> Epoch: 83 Train set loss: KLD Loss: 0.002947 \t NLL Loss: 0.793687 \t MMD Loss: 0.000058 \t Average loss: 0.7966\n",
      "====> Test set loss: KLD Loss = 0.0009 \t NLL Loss = 0.6349 \t MMD Loss = 0.0000\n",
      "Train Epoch: 84 Batch: 0 KLD Loss: 0.004521 \t NLL Loss: 0.952532 \t MMD Loss: 0.000062\n",
      "Train Epoch: 84 Batch: 1 KLD Loss: 0.001138 \t NLL Loss: 0.707728 \t MMD Loss: 0.000062\n",
      "Train Epoch: 84 Batch: 2 KLD Loss: 0.001560 \t NLL Loss: 0.839783 \t MMD Loss: 0.000060\n",
      "Train Epoch: 84 Batch: 3 KLD Loss: 0.007494 \t NLL Loss: 0.814423 \t MMD Loss: 0.000058\n",
      "Train Epoch: 84 Batch: 4 KLD Loss: 0.006647 \t NLL Loss: 0.808802 \t MMD Loss: 0.000058\n",
      "Train Epoch: 84 Batch: 5 KLD Loss: 0.001544 \t NLL Loss: 0.847541 \t MMD Loss: 0.000059\n",
      "====> Epoch: 84 Train set loss: KLD Loss: 0.003655 \t NLL Loss: 0.793346 \t MMD Loss: 0.000057 \t Average loss: 0.7969\n",
      "====> Test set loss: KLD Loss = 0.0008 \t NLL Loss = 0.6344 \t MMD Loss = 0.0000\n",
      "Train Epoch: 85 Batch: 0 KLD Loss: 0.004898 \t NLL Loss: 0.952821 \t MMD Loss: 0.000062\n",
      "Train Epoch: 85 Batch: 1 KLD Loss: 0.006624 \t NLL Loss: 0.707500 \t MMD Loss: 0.000063\n",
      "Train Epoch: 85 Batch: 2 KLD Loss: 0.003047 \t NLL Loss: 0.839322 \t MMD Loss: 0.000062\n",
      "Train Epoch: 85 Batch: 3 KLD Loss: 0.000303 \t NLL Loss: 0.813348 \t MMD Loss: 0.000061\n",
      "Train Epoch: 85 Batch: 4 KLD Loss: 0.000941 \t NLL Loss: 0.808410 \t MMD Loss: 0.000060\n",
      "Train Epoch: 85 Batch: 5 KLD Loss: 0.002365 \t NLL Loss: 0.846577 \t MMD Loss: 0.000059\n",
      "====> Epoch: 85 Train set loss: KLD Loss: 0.002901 \t NLL Loss: 0.792894 \t MMD Loss: 0.000059 \t Average loss: 0.7957\n",
      "====> Test set loss: KLD Loss = 0.0012 \t NLL Loss = 0.6337 \t MMD Loss = 0.0000\n",
      "Train Epoch: 86 Batch: 0 KLD Loss: 0.009312 \t NLL Loss: 0.953245 \t MMD Loss: 0.000059\n",
      "Train Epoch: 86 Batch: 1 KLD Loss: 0.002956 \t NLL Loss: 0.707386 \t MMD Loss: 0.000059\n",
      "Train Epoch: 86 Batch: 2 KLD Loss: 0.001601 \t NLL Loss: 0.838726 \t MMD Loss: 0.000061\n",
      "Train Epoch: 86 Batch: 3 KLD Loss: 0.009568 \t NLL Loss: 0.812352 \t MMD Loss: 0.000064\n",
      "Train Epoch: 86 Batch: 4 KLD Loss: 0.009842 \t NLL Loss: 0.808060 \t MMD Loss: 0.000064\n",
      "Train Epoch: 86 Batch: 5 KLD Loss: 0.001626 \t NLL Loss: 0.845754 \t MMD Loss: 0.000062\n",
      "====> Epoch: 86 Train set loss: KLD Loss: 0.005571 \t NLL Loss: 0.792503 \t MMD Loss: 0.000059 \t Average loss: 0.7980\n",
      "====> Test set loss: KLD Loss = 0.0009 \t NLL Loss = 0.6333 \t MMD Loss = 0.0000\n",
      "Train Epoch: 87 Batch: 0 KLD Loss: 0.004428 \t NLL Loss: 0.953651 \t MMD Loss: 0.000059\n",
      "Train Epoch: 87 Batch: 1 KLD Loss: 0.007342 \t NLL Loss: 0.706950 \t MMD Loss: 0.000058\n",
      "Train Epoch: 87 Batch: 2 KLD Loss: 0.003532 \t NLL Loss: 0.838440 \t MMD Loss: 0.000059\n",
      "Train Epoch: 87 Batch: 3 KLD Loss: 0.001195 \t NLL Loss: 0.811358 \t MMD Loss: 0.000060\n",
      "Train Epoch: 87 Batch: 4 KLD Loss: 0.000366 \t NLL Loss: 0.807747 \t MMD Loss: 0.000061\n",
      "Train Epoch: 87 Batch: 5 KLD Loss: 0.001868 \t NLL Loss: 0.844855 \t MMD Loss: 0.000062\n",
      "====> Epoch: 87 Train set loss: KLD Loss: 0.002989 \t NLL Loss: 0.792100 \t MMD Loss: 0.000057 \t Average loss: 0.7950\n",
      "====> Test set loss: KLD Loss = 0.0018 \t NLL Loss = 0.6326 \t MMD Loss = 0.0000\n",
      "Train Epoch: 88 Batch: 0 KLD Loss: 0.016504 \t NLL Loss: 0.954820 \t MMD Loss: 0.000064\n",
      "Train Epoch: 88 Batch: 1 KLD Loss: 0.006379 \t NLL Loss: 0.706642 \t MMD Loss: 0.000064\n",
      "Train Epoch: 88 Batch: 2 KLD Loss: 0.001455 \t NLL Loss: 0.837992 \t MMD Loss: 0.000061\n",
      "Train Epoch: 88 Batch: 3 KLD Loss: 0.012986 \t NLL Loss: 0.810605 \t MMD Loss: 0.000057\n",
      "Train Epoch: 88 Batch: 4 KLD Loss: 0.014818 \t NLL Loss: 0.807325 \t MMD Loss: 0.000057\n",
      "Train Epoch: 88 Batch: 5 KLD Loss: 0.004256 \t NLL Loss: 0.844066 \t MMD Loss: 0.000059\n",
      "====> Epoch: 88 Train set loss: KLD Loss: 0.009001 \t NLL Loss: 0.791852 \t MMD Loss: 0.000058 \t Average loss: 0.8008\n",
      "====> Test set loss: KLD Loss = 0.0006 \t NLL Loss = 0.6322 \t MMD Loss = 0.0000\n",
      "Train Epoch: 89 Batch: 0 KLD Loss: 0.002204 \t NLL Loss: 0.954690 \t MMD Loss: 0.000062\n",
      "Train Epoch: 89 Batch: 1 KLD Loss: 0.007026 \t NLL Loss: 0.706333 \t MMD Loss: 0.000063\n",
      "Train Epoch: 89 Batch: 2 KLD Loss: 0.007288 \t NLL Loss: 0.837542 \t MMD Loss: 0.000063\n",
      "Train Epoch: 89 Batch: 3 KLD Loss: 0.003766 \t NLL Loss: 0.809547 \t MMD Loss: 0.000063\n",
      "Train Epoch: 89 Batch: 4 KLD Loss: 0.000571 \t NLL Loss: 0.807002 \t MMD Loss: 0.000061\n",
      "Train Epoch: 89 Batch: 5 KLD Loss: 0.002156 \t NLL Loss: 0.843125 \t MMD Loss: 0.000060\n",
      "====> Epoch: 89 Train set loss: KLD Loss: 0.003673 \t NLL Loss: 0.791340 \t MMD Loss: 0.000059 \t Average loss: 0.7950\n",
      "====> Test set loss: KLD Loss = 0.0027 \t NLL Loss = 0.6315 \t MMD Loss = 0.0000\n",
      "Train Epoch: 90 Batch: 0 KLD Loss: 0.027045 \t NLL Loss: 0.955045 \t MMD Loss: 0.000057\n",
      "Train Epoch: 90 Batch: 1 KLD Loss: 0.013279 \t NLL Loss: 0.706258 \t MMD Loss: 0.000057\n",
      "Train Epoch: 90 Batch: 2 KLD Loss: 0.001137 \t NLL Loss: 0.836944 \t MMD Loss: 0.000060\n",
      "Train Epoch: 90 Batch: 3 KLD Loss: 0.014222 \t NLL Loss: 0.808601 \t MMD Loss: 0.000064\n",
      "Train Epoch: 90 Batch: 4 KLD Loss: 0.019150 \t NLL Loss: 0.806663 \t MMD Loss: 0.000065\n",
      "Train Epoch: 90 Batch: 5 KLD Loss: 0.005328 \t NLL Loss: 0.842312 \t MMD Loss: 0.000063\n",
      "====> Epoch: 90 Train set loss: KLD Loss: 0.012794 \t NLL Loss: 0.790954 \t MMD Loss: 0.000059 \t Average loss: 0.8037\n",
      "====> Test set loss: KLD Loss = 0.0005 \t NLL Loss = 0.6311 \t MMD Loss = 0.0000\n",
      "Train Epoch: 91 Batch: 0 KLD Loss: 0.000720 \t NLL Loss: 0.955681 \t MMD Loss: 0.000060\n",
      "Train Epoch: 91 Batch: 1 KLD Loss: 0.005906 \t NLL Loss: 0.705719 \t MMD Loss: 0.000059\n",
      "Train Epoch: 91 Batch: 2 KLD Loss: 0.008881 \t NLL Loss: 0.837035 \t MMD Loss: 0.000058\n",
      "Train Epoch: 91 Batch: 3 KLD Loss: 0.010101 \t NLL Loss: 0.807743 \t MMD Loss: 0.000058\n",
      "Train Epoch: 91 Batch: 4 KLD Loss: 0.002804 \t NLL Loss: 0.806226 \t MMD Loss: 0.000059\n",
      "Train Epoch: 91 Batch: 5 KLD Loss: 0.001402 \t NLL Loss: 0.841434 \t MMD Loss: 0.000061\n",
      "====> Epoch: 91 Train set loss: KLD Loss: 0.004759 \t NLL Loss: 0.790637 \t MMD Loss: 0.000057 \t Average loss: 0.7953\n",
      "====> Test set loss: KLD Loss = 0.0038 \t NLL Loss = 0.6304 \t MMD Loss = 0.0000\n",
      "Saved model to saves/vrnn_state_dict_91.pth\n",
      "Train Epoch: 92 Batch: 0 KLD Loss: 0.039310 \t NLL Loss: 0.957053 \t MMD Loss: 0.000066\n",
      "Train Epoch: 92 Batch: 1 KLD Loss: 0.022336 \t NLL Loss: 0.705787 \t MMD Loss: 0.000066\n",
      "Train Epoch: 92 Batch: 2 KLD Loss: 0.001873 \t NLL Loss: 0.836181 \t MMD Loss: 0.000062\n",
      "Train Epoch: 92 Batch: 3 KLD Loss: 0.014940 \t NLL Loss: 0.806941 \t MMD Loss: 0.000057\n",
      "Train Epoch: 92 Batch: 4 KLD Loss: 0.024438 \t NLL Loss: 0.805927 \t MMD Loss: 0.000056\n",
      "Train Epoch: 92 Batch: 5 KLD Loss: 0.011153 \t NLL Loss: 0.840803 \t MMD Loss: 0.000057\n",
      "====> Epoch: 92 Train set loss: KLD Loss: 0.018203 \t NLL Loss: 0.790454 \t MMD Loss: 0.000058 \t Average loss: 0.8086\n",
      "====> Test set loss: KLD Loss = 0.0004 \t NLL Loss = 0.6301 \t MMD Loss = 0.0000\n",
      "Train Epoch: 93 Batch: 0 KLD Loss: 0.000594 \t NLL Loss: 0.956478 \t MMD Loss: 0.000060\n",
      "Train Epoch: 93 Batch: 1 KLD Loss: 0.003207 \t NLL Loss: 0.705282 \t MMD Loss: 0.000062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 Batch: 2 KLD Loss: 0.013575 \t NLL Loss: 0.835929 \t MMD Loss: 0.000064\n",
      "Train Epoch: 93 Batch: 3 KLD Loss: 0.019387 \t NLL Loss: 0.805880 \t MMD Loss: 0.000065\n",
      "Train Epoch: 93 Batch: 4 KLD Loss: 0.008313 \t NLL Loss: 0.805622 \t MMD Loss: 0.000064\n",
      "Train Epoch: 93 Batch: 5 KLD Loss: 0.001281 \t NLL Loss: 0.839776 \t MMD Loss: 0.000061\n",
      "====> Epoch: 93 Train set loss: KLD Loss: 0.007398 \t NLL Loss: 0.789860 \t MMD Loss: 0.000060 \t Average loss: 0.7972\n",
      "====> Test set loss: KLD Loss = 0.0047 \t NLL Loss = 0.6293 \t MMD Loss = 0.0000\n",
      "Train Epoch: 94 Batch: 0 KLD Loss: 0.048024 \t NLL Loss: 0.956904 \t MMD Loss: 0.000056\n",
      "Train Epoch: 94 Batch: 1 KLD Loss: 0.034891 \t NLL Loss: 0.705314 \t MMD Loss: 0.000055\n",
      "Train Epoch: 94 Batch: 2 KLD Loss: 0.003105 \t NLL Loss: 0.835421 \t MMD Loss: 0.000059\n",
      "Train Epoch: 94 Batch: 3 KLD Loss: 0.009287 \t NLL Loss: 0.805013 \t MMD Loss: 0.000064\n",
      "Train Epoch: 94 Batch: 4 KLD Loss: 0.023708 \t NLL Loss: 0.805266 \t MMD Loss: 0.000066\n",
      "Train Epoch: 94 Batch: 5 KLD Loss: 0.012850 \t NLL Loss: 0.838949 \t MMD Loss: 0.000064\n",
      "====> Epoch: 94 Train set loss: KLD Loss: 0.021046 \t NLL Loss: 0.789525 \t MMD Loss: 0.000058 \t Average loss: 0.8105\n",
      "====> Test set loss: KLD Loss = 0.0010 \t NLL Loss = 0.6289 \t MMD Loss = 0.0000\n",
      "Train Epoch: 95 Batch: 0 KLD Loss: 0.005020 \t NLL Loss: 0.957937 \t MMD Loss: 0.000063\n",
      "Train Epoch: 95 Batch: 1 KLD Loss: 0.000884 \t NLL Loss: 0.704587 \t MMD Loss: 0.000061\n",
      "Train Epoch: 95 Batch: 2 KLD Loss: 0.012399 \t NLL Loss: 0.835749 \t MMD Loss: 0.000058\n",
      "Train Epoch: 95 Batch: 3 KLD Loss: 0.030559 \t NLL Loss: 0.804575 \t MMD Loss: 0.000056\n",
      "Train Epoch: 95 Batch: 4 KLD Loss: 0.019533 \t NLL Loss: 0.804953 \t MMD Loss: 0.000056\n",
      "Train Epoch: 95 Batch: 5 KLD Loss: 0.002315 \t NLL Loss: 0.838205 \t MMD Loss: 0.000059\n",
      "====> Epoch: 95 Train set loss: KLD Loss: 0.011285 \t NLL Loss: 0.789387 \t MMD Loss: 0.000056 \t Average loss: 0.8006\n",
      "====> Test set loss: KLD Loss = 0.0037 \t NLL Loss = 0.6283 \t MMD Loss = 0.0000\n",
      "Train Epoch: 96 Batch: 0 KLD Loss: 0.045871 \t NLL Loss: 0.958654 \t MMD Loss: 0.000066\n",
      "Train Epoch: 96 Batch: 1 KLD Loss: 0.040139 \t NLL Loss: 0.704928 \t MMD Loss: 0.000068\n",
      "Train Epoch: 96 Batch: 2 KLD Loss: 0.007520 \t NLL Loss: 0.834640 \t MMD Loss: 0.000064\n",
      "Train Epoch: 96 Batch: 3 KLD Loss: 0.004299 \t NLL Loss: 0.803294 \t MMD Loss: 0.000059\n",
      "Train Epoch: 96 Batch: 4 KLD Loss: 0.016944 \t NLL Loss: 0.804587 \t MMD Loss: 0.000057\n",
      "Train Epoch: 96 Batch: 5 KLD Loss: 0.015267 \t NLL Loss: 0.837419 \t MMD Loss: 0.000057\n",
      "====> Epoch: 96 Train set loss: KLD Loss: 0.020755 \t NLL Loss: 0.788991 \t MMD Loss: 0.000059 \t Average loss: 0.8097\n",
      "====> Test set loss: KLD Loss = 0.0024 \t NLL Loss = 0.6279 \t MMD Loss = 0.0000\n",
      "Train Epoch: 97 Batch: 0 KLD Loss: 0.013075 \t NLL Loss: 0.958264 \t MMD Loss: 0.000058\n",
      "Train Epoch: 97 Batch: 1 KLD Loss: 0.001404 \t NLL Loss: 0.704388 \t MMD Loss: 0.000060\n",
      "Train Epoch: 97 Batch: 2 KLD Loss: 0.009108 \t NLL Loss: 0.834480 \t MMD Loss: 0.000063\n",
      "Train Epoch: 97 Batch: 3 KLD Loss: 0.032203 \t NLL Loss: 0.802511 \t MMD Loss: 0.000067\n",
      "Train Epoch: 97 Batch: 4 KLD Loss: 0.027066 \t NLL Loss: 0.804281 \t MMD Loss: 0.000066\n",
      "Train Epoch: 97 Batch: 5 KLD Loss: 0.003943 \t NLL Loss: 0.836542 \t MMD Loss: 0.000063\n",
      "====> Epoch: 97 Train set loss: KLD Loss: 0.013853 \t NLL Loss: 0.788503 \t MMD Loss: 0.000060 \t Average loss: 0.8023\n",
      "====> Test set loss: KLD Loss = 0.0025 \t NLL Loss = 0.6273 \t MMD Loss = 0.0000\n",
      "Train Epoch: 98 Batch: 0 KLD Loss: 0.030239 \t NLL Loss: 0.958780 \t MMD Loss: 0.000057\n",
      "Train Epoch: 98 Batch: 1 KLD Loss: 0.036601 \t NLL Loss: 0.704248 \t MMD Loss: 0.000055\n",
      "Train Epoch: 98 Batch: 2 KLD Loss: 0.011048 \t NLL Loss: 0.834103 \t MMD Loss: 0.000057\n",
      "Train Epoch: 98 Batch: 3 KLD Loss: 0.000367 \t NLL Loss: 0.801645 \t MMD Loss: 0.000061\n",
      "Train Epoch: 98 Batch: 4 KLD Loss: 0.006672 \t NLL Loss: 0.804033 \t MMD Loss: 0.000063\n",
      "Train Epoch: 98 Batch: 5 KLD Loss: 0.010131 \t NLL Loss: 0.835753 \t MMD Loss: 0.000064\n",
      "====> Epoch: 98 Train set loss: KLD Loss: 0.015171 \t NLL Loss: 0.788199 \t MMD Loss: 0.000057 \t Average loss: 0.8033\n",
      "====> Test set loss: KLD Loss = 0.0044 \t NLL Loss = 0.6268 \t MMD Loss = 0.0000\n",
      "Train Epoch: 99 Batch: 0 KLD Loss: 0.021764 \t NLL Loss: 0.960088 \t MMD Loss: 0.000065\n",
      "Train Epoch: 99 Batch: 1 KLD Loss: 0.005545 \t NLL Loss: 0.703884 \t MMD Loss: 0.000063\n",
      "Train Epoch: 99 Batch: 2 KLD Loss: 0.003746 \t NLL Loss: 0.834117 \t MMD Loss: 0.000060\n",
      "Train Epoch: 99 Batch: 3 KLD Loss: 0.025136 \t NLL Loss: 0.801254 \t MMD Loss: 0.000056\n",
      "Train Epoch: 99 Batch: 4 KLD Loss: 0.028237 \t NLL Loss: 0.803715 \t MMD Loss: 0.000055\n",
      "Train Epoch: 99 Batch: 5 KLD Loss: 0.009221 \t NLL Loss: 0.835169 \t MMD Loss: 0.000058\n",
      "====> Epoch: 99 Train set loss: KLD Loss: 0.014946 \t NLL Loss: 0.788146 \t MMD Loss: 0.000057 \t Average loss: 0.8030\n",
      "====> Test set loss: KLD Loss = 0.0004 \t NLL Loss = 0.6263 \t MMD Loss = 0.0000\n",
      "Train Epoch: 100 Batch: 0 KLD Loss: 0.011287 \t NLL Loss: 0.959741 \t MMD Loss: 0.000063\n",
      "Train Epoch: 100 Batch: 1 KLD Loss: 0.019558 \t NLL Loss: 0.703656 \t MMD Loss: 0.000065\n",
      "Train Epoch: 100 Batch: 2 KLD Loss: 0.011000 \t NLL Loss: 0.833257 \t MMD Loss: 0.000064\n",
      "Train Epoch: 100 Batch: 3 KLD Loss: 0.001271 \t NLL Loss: 0.799968 \t MMD Loss: 0.000062\n",
      "Train Epoch: 100 Batch: 4 KLD Loss: 0.000791 \t NLL Loss: 0.803410 \t MMD Loss: 0.000060\n",
      "Train Epoch: 100 Batch: 5 KLD Loss: 0.004264 \t NLL Loss: 0.834218 \t MMD Loss: 0.000059\n",
      "====> Epoch: 100 Train set loss: KLD Loss: 0.007688 \t NLL Loss: 0.787511 \t MMD Loss: 0.000060 \t Average loss: 0.7951\n",
      "====> Test set loss: KLD Loss = 0.0029 \t NLL Loss = 0.6257 \t MMD Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    #training + testing\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    #saving model\n",
    "    if epoch % save_every == 1:\n",
    "        fn = 'saves/vrnn_state_dict_'+ str(epoch)+'.pth'\n",
    "        checkpoint = { 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict()}\n",
    "        torch.save(checkpoint, fn)\n",
    "        print('Saved model to '+ fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentation] *",
   "language": "python",
   "name": "conda-env-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
